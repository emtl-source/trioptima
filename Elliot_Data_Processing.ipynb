{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 1 - PROCESS CLEAN DATA (TRAINING) AND SAVE THE SCALER AS robust_scaler.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gusta\\AppData\\Local\\Temp\\ipykernel_18448\\604178254.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce')\n",
      "C:\\Users\\gusta\\AppData\\Local\\Temp\\ipykernel_18448\\604178254.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with a single unique value:  ['customBasketIndicator', 'packageIndicator', 'postPricedSwapIndicator', 'effectiveDate_hour', 'effectiveDate_minute', 'effectiveDate_second', 'eventDateTime_year', 'eventDateTime_month', 'eventDateTime_day', 'eventDateTime_hour', 'eventDateTime_minute', 'eventDateTime_second', 'eventDateTime_weekday', 'executionDateTime_year', 'executionDateTime_month', 'executionDateTime_day', 'executionDateTime_hour', 'executionDateTime_minute', 'executionDateTime_second', 'executionDateTime_weekday', 'expirationDate_hour', 'expirationDate_minute', 'expirationDate_second']\n",
      "Columns with NaN or infinite values before scaling:  []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib\n",
    "\n",
    "#r'C:\\Users\\gusta\\Documents\\KTH\\TriOptima\\trioptima'\n",
    "#'/Users/elliotlindestam/Documents/Skola/Indek icloud/trioptima/'\n",
    "your_path = r'C:\\Users\\gusta\\Documents\\KTH\\TriOptima\\trioptima/'\n",
    "training_data_name = 'RT.IRS_Data_2023_Cleaned'\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(your_path + training_data_name+'.csv')\n",
    "\n",
    "# Step 1: Handle Missing Data\n",
    "data_filled = data.fillna(0)\n",
    "\n",
    "# Step 2: Extract Information from DateTime Columns\n",
    "datetime_columns = ['effectiveDate', 'eventDateTime', 'executionDateTime', 'expirationDate']\n",
    "\n",
    "def extract_date_features(df, column):\n",
    "    df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "    df[column + '_year'] = df[column].dt.year\n",
    "    df[column + '_month'] = df[column].dt.month\n",
    "    df[column + '_day'] = df[column].dt.day\n",
    "    df[column + '_hour'] = df[column].dt.hour\n",
    "    df[column + '_minute'] = df[column].dt.minute\n",
    "    df[column + '_second'] = df[column].dt.second\n",
    "    df[column + '_weekday'] = df[column].dt.weekday\n",
    "    # Drop the original datetime column\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df\n",
    "\n",
    "for col in datetime_columns:\n",
    "    data_filled = extract_date_features(data_filled, col)\n",
    "\n",
    "original_dtypes = data.dtypes.to_dict()\n",
    "\n",
    "# Convert boolean columns to binary (1/0) before one-hot encoding other categorical columns\n",
    "data_filled = data_filled*1\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Encode Categorical Variables\n",
    "categorical_columns = data_filled.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "categorical_columns = [col for col in categorical_columns if col not in datetime_columns]\n",
    "data_encoded = pd.get_dummies(data_filled, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Step 3.5: Identify and Drop Single-Value Columns\n",
    "single_value_columns = data_encoded.columns[data_encoded.nunique() == 1].tolist()\n",
    "\n",
    "# Optionally print these columns and their unique values\n",
    "print(\"Columns with a single unique value: \", single_value_columns)\n",
    "\n",
    "# Drop the single-value columns\n",
    "data_encoded = data_encoded.drop(columns=single_value_columns)\n",
    "\n",
    "# Define numerical_columns here, after all column dropping and adding has occurred\n",
    "numerical_columns = data_encoded.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Check for NaN or infinite values in numerical columns before scaling\n",
    "nan_inf_columns = data_encoded[numerical_columns].columns[data_encoded[numerical_columns].isna().any() | np.isinf(data_encoded[numerical_columns]).any()].tolist()\n",
    "print(\"Columns with NaN or infinite values before scaling: \", nan_inf_columns)\n",
    "\n",
    "# Step 4: Scale/Normalize Data\n",
    "numerical_columns = data_encoded.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "scaler = RobustScaler()\n",
    "data_encoded[numerical_columns] = scaler.fit_transform(data_encoded[numerical_columns])\n",
    "\n",
    "# Save the scaler for future use\n",
    "scaler_filename = 'robust_scaler.pkl'\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "# Preprocessed data is now stored in `data_encoded`, and the scaler is saved as `robust_scaler.pkl`\n",
    "\n",
    "# Save the processed data to a CSV file in this environment\n",
    "data_encoded.to_csv(your_path+training_data_name+'_Scaled.csv', index=False)\n",
    "\n",
    "train_encoded_columns = data_encoded.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2 - PROCESS THE TEST DATA USING THE SCALER IN TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with a single unique value:  ['customBasketIndicator', 'nonStandardTermIndicator', 'packageIndicator', 'platformID', 'postPricedSwapIndicator', 'primeBrokerageTransactionIndicator', 'effectiveDate_hour', 'effectiveDate_minute', 'effectiveDate_second', 'eventDateTime_year', 'eventDateTime_month', 'eventDateTime_day', 'eventDateTime_hour', 'eventDateTime_minute', 'eventDateTime_second', 'eventDateTime_weekday', 'executionDateTime_year', 'executionDateTime_month', 'executionDateTime_day', 'executionDateTime_hour', 'executionDateTime_minute', 'executionDateTime_second', 'executionDateTime_weekday', 'expirationDate_hour', 'expirationDate_minute', 'expirationDate_second']\n",
      "Columns with NaN or infinite values before scaling:  []\n",
      "All columns in t_data_matched are equal to train_encoded_columns.\n",
      "    leg1NotionalAmount  leg2NotionalAmount  leg2ResetFrequencyMultiplier  \\\n",
      "0            -0.170909           -0.170909                           0.0   \n",
      "1             0.083636            0.083636                           0.0   \n",
      "2            -0.160000           -0.160000                           0.0   \n",
      "3            -0.170909           -0.170909                           0.0   \n",
      "4            -0.217818           -0.217818                           0.0   \n",
      "5             0.138182            0.138182                           0.0   \n",
      "6            -0.196364           -0.196364                           0.0   \n",
      "7             0.181818            0.181818                           0.0   \n",
      "8            -0.112727           -0.112727                           0.0   \n",
      "9             7.054545            7.054545                          -0.4   \n",
      "10            7.054545            7.054545                          -0.4   \n",
      "11            7.054545            7.054545                          -0.4   \n",
      "12            0.363636            0.363636                           0.6   \n",
      "13           14.327273           14.327273                          -0.4   \n",
      "14            0.436364            0.436364                           0.6   \n",
      "15            0.218182            0.218182                           0.6   \n",
      "16            7.054545            7.054545                          -0.4   \n",
      "17            2.509091            2.509091                          -0.4   \n",
      "18            1.272727            1.272727                          -0.4   \n",
      "19           -0.072727           -0.072727                           0.0   \n",
      "\n",
      "    leg1FixedRate  leg1FixedRatePaymentFrequencyMultiplier  \\\n",
      "0       -0.654535                                      1.0   \n",
      "1       -0.510110                                      1.0   \n",
      "2       -0.510110                                      1.0   \n",
      "3       -0.654535                                      1.0   \n",
      "4       -0.654535                                      1.0   \n",
      "5       -0.510110                                      1.0   \n",
      "6        2.196418                                      0.4   \n",
      "7        0.195263                                      1.0   \n",
      "8       -0.319469                                      0.0   \n",
      "9       -0.169266                                      0.0   \n",
      "10      -0.112940                                      0.0   \n",
      "11      -0.232813                                      0.0   \n",
      "12      -1.403813                                      0.0   \n",
      "13       0.532062                                      0.0   \n",
      "14      -1.406701                                      0.0   \n",
      "15      -1.736568                                      0.0   \n",
      "16       0.362218                                      0.0   \n",
      "17      -0.105719                                      0.0   \n",
      "18      -0.089832                                      0.0   \n",
      "19      -0.244367                                      0.0   \n",
      "\n",
      "    leg2FloatingRatePaymentFrequencyMultiplier  leg2UnderlierTenorMultiplier  \\\n",
      "0                                          0.0                      0.333333   \n",
      "1                                          0.0                      0.333333   \n",
      "2                                          0.0                      0.333333   \n",
      "3                                          0.0                      0.333333   \n",
      "4                                          0.0                      0.333333   \n",
      "5                                          0.0                      0.333333   \n",
      "6                                          0.0                      0.666667   \n",
      "7                                          0.0                      0.000000   \n",
      "8                                          0.0                      1.000000   \n",
      "9                                         -0.4                     -0.333333   \n",
      "10                                        -0.4                     -0.333333   \n",
      "11                                        -0.4                     -0.333333   \n",
      "12                                         0.6                      2.666667   \n",
      "13                                        -0.4                      0.000000   \n",
      "14                                         0.6                      2.666667   \n",
      "15                                         0.6                      6.000000   \n",
      "16                                        -0.4                     -0.333333   \n",
      "17                                        -0.4                     -0.333333   \n",
      "18                                        -0.4                      0.000000   \n",
      "19                                         0.0                      2.000000   \n",
      "\n",
      "    effectiveDate_year  effectiveDate_month  effectiveDate_day  ...  \\\n",
      "0                 2022                   12                 21  ...   \n",
      "1                 2022                   12                 21  ...   \n",
      "2                 2022                   12                 21  ...   \n",
      "3                 2022                   12                 21  ...   \n",
      "4                 2022                   12                 21  ...   \n",
      "5                 2022                   12                 21  ...   \n",
      "6                 2023                   12                 20  ...   \n",
      "7                 2023                    1                 30  ...   \n",
      "8                 2023                    2                  1  ...   \n",
      "9                 2023                    4                  5  ...   \n",
      "10                2023                    5                  3  ...   \n",
      "11                2023                    3                  8  ...   \n",
      "12                2041                   11                 11  ...   \n",
      "13                2023                    3                 22  ...   \n",
      "14                2041                   11                 11  ...   \n",
      "15                2052                    4                 15  ...   \n",
      "16                2023                    3                  9  ...   \n",
      "17                2023                    5                  3  ...   \n",
      "18                2023                   12                  6  ...   \n",
      "19                2023                    4                 19  ...   \n",
      "\n",
      "    leg2UnderlierIDSource_TELBOR  leg2UnderlierIDSource_TIIE  \\\n",
      "0                          False                           0   \n",
      "1                          False                           0   \n",
      "2                          False                           0   \n",
      "3                          False                           0   \n",
      "4                          False                           0   \n",
      "5                          False                           0   \n",
      "6                          False                           0   \n",
      "7                          False                           0   \n",
      "8                           True                           0   \n",
      "9                          False                           0   \n",
      "10                         False                           0   \n",
      "11                         False                           0   \n",
      "12                         False                           0   \n",
      "13                         False                           0   \n",
      "14                         False                           0   \n",
      "15                         False                           0   \n",
      "16                         False                           0   \n",
      "17                         False                           0   \n",
      "18                         False                           0   \n",
      "19                         False                           0   \n",
      "\n",
      "    leg2UnderlierIDSource_U  leg2UnderlierIDSource_WIBOR  \\\n",
      "0                         0                            0   \n",
      "1                         0                            0   \n",
      "2                         0                            0   \n",
      "3                         0                            0   \n",
      "4                         0                            0   \n",
      "5                         0                            0   \n",
      "6                         0                            0   \n",
      "7                         0                            0   \n",
      "8                         0                            0   \n",
      "9                         0                            0   \n",
      "10                        0                            0   \n",
      "11                        0                            0   \n",
      "12                        0                            0   \n",
      "13                        0                            0   \n",
      "14                        0                            0   \n",
      "15                        0                            0   \n",
      "16                        0                            0   \n",
      "17                        0                            0   \n",
      "18                        0                            0   \n",
      "19                        0                            0   \n",
      "\n",
      "    leg2UnderlierTenorPeriod_MNTH  leg2UnderlierTenorPeriod_YEAR  \\\n",
      "0                               0                          False   \n",
      "1                               0                          False   \n",
      "2                               0                          False   \n",
      "3                               0                          False   \n",
      "4                               0                          False   \n",
      "5                               0                          False   \n",
      "6                               0                           True   \n",
      "7                               0                           True   \n",
      "8                               0                           True   \n",
      "9                               0                          False   \n",
      "10                              0                          False   \n",
      "11                              0                          False   \n",
      "12                              0                           True   \n",
      "13                              0                          False   \n",
      "14                              0                           True   \n",
      "15                              0                           True   \n",
      "16                              0                          False   \n",
      "17                              0                          False   \n",
      "18                              0                          False   \n",
      "19                              0                          False   \n",
      "\n",
      "    leg2ResetFrequencyPeriod_DAIL  leg2ResetFrequencyPeriod_MNTH  \\\n",
      "0                               0                           True   \n",
      "1                               0                           True   \n",
      "2                               0                           True   \n",
      "3                               0                           True   \n",
      "4                               0                           True   \n",
      "5                               0                           True   \n",
      "6                               0                           True   \n",
      "7                               0                           True   \n",
      "8                               0                           True   \n",
      "9                               0                          False   \n",
      "10                              0                          False   \n",
      "11                              0                          False   \n",
      "12                              0                           True   \n",
      "13                              0                          False   \n",
      "14                              0                           True   \n",
      "15                              0                           True   \n",
      "16                              0                          False   \n",
      "17                              0                          False   \n",
      "18                              0                          False   \n",
      "19                              0                           True   \n",
      "\n",
      "    leg2ResetFrequencyPeriod_WEEK  leg2ResetFrequencyPeriod_YEAR  \n",
      "0                               0                              0  \n",
      "1                               0                              0  \n",
      "2                               0                              0  \n",
      "3                               0                              0  \n",
      "4                               0                              0  \n",
      "5                               0                              0  \n",
      "6                               0                              0  \n",
      "7                               0                              0  \n",
      "8                               0                              0  \n",
      "9                               0                              0  \n",
      "10                              0                              0  \n",
      "11                              0                              0  \n",
      "12                              0                              0  \n",
      "13                              0                              0  \n",
      "14                              0                              0  \n",
      "15                              0                              0  \n",
      "16                              0                              0  \n",
      "17                              0                              0  \n",
      "18                              0                              0  \n",
      "19                              0                              0  \n",
      "\n",
      "[20 rows x 495 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gusta\\AppData\\Local\\Temp\\ipykernel_18448\\2866336308.py:25: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce')\n",
      "C:\\Users\\gusta\\AppData\\Local\\Temp\\ipykernel_18448\\2866336308.py:25: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[column] = pd.to_datetime(df[column], errors='coerce')\n",
      "C:\\Users\\gusta\\AppData\\Local\\Temp\\ipykernel_18448\\2866336308.py:80: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(original_dtype):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib\n",
    "\n",
    "\n",
    "#r'C:\\Users\\gusta\\Documents\\KTH\\TriOptima\\trioptima/'\n",
    "#'/Users/elliotlindestam/Documents/Skola/Indek icloud/trioptima/'\n",
    "your_path = r'C:\\Users\\gusta\\Documents\\KTH\\TriOptima\\trioptima/'\n",
    "test_data_name = 'RT.IRS_Data_2023_Cleaned_Test'\n",
    "\n",
    "# Load the data\n",
    "t_data = pd.read_csv(your_path + test_data_name+'.csv')\n",
    "\n",
    "# Drop 'disseminationTimestamp' and its derived columns\n",
    "#t_data = t_data.drop(columns=['disseminationTimestamp','sDRreceiptTimestamp','disseminationIdentifier'])\n",
    "\n",
    "# Step 1: Handle Missing Data\n",
    "t_data_filled = t_data.fillna(0)\n",
    "\n",
    "# Step 2: Extract Information from DateTime Columns\n",
    "t_datetime_columns = ['effectiveDate', 'eventDateTime', 'executionDateTime', 'expirationDate']\n",
    "\n",
    "def extract_date_features(df, column):\n",
    "    df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "    df[column + '_year'] = df[column].dt.year\n",
    "    df[column + '_month'] = df[column].dt.month\n",
    "    df[column + '_day'] = df[column].dt.day\n",
    "    df[column + '_hour'] = df[column].dt.hour\n",
    "    df[column + '_minute'] = df[column].dt.minute\n",
    "    df[column + '_second'] = df[column].dt.second\n",
    "    df[column + '_weekday'] = df[column].dt.weekday\n",
    "    # Drop the original datetime column\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df\n",
    "\n",
    "for col in t_datetime_columns:\n",
    "    t_data_filled = extract_date_features(t_data_filled, col)\n",
    "\n",
    "# Convert boolean columns to binary (1/0) before one-hot encoding other categorical columns\n",
    "t_data_filled = t_data_filled*1\n",
    "\n",
    "# Step 3: Encode Categorical Variables\n",
    "t_categorical_columns = t_data_filled.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "t_categorical_columns = [col for col in t_categorical_columns if col not in t_datetime_columns]\n",
    "t_data_encoded = pd.get_dummies(t_data_filled, columns=t_categorical_columns, drop_first=True)\n",
    "\n",
    "# Step 3.5: Identify and Drop Single-Value Columns\n",
    "t_single_value_columns = t_data_encoded.columns[t_data_encoded.nunique() == 1].tolist()\n",
    "\n",
    "# Optionally print these columns and their unique values\n",
    "print(\"Columns with a single unique value: \", t_single_value_columns)\n",
    "\n",
    "# Drop the single-value columns\n",
    "t_data_encoded = t_data_encoded.drop(columns=t_single_value_columns)\n",
    "\n",
    "# Define numerical_columns here, after all column dropping and adding has occurred\n",
    "t_numerical_columns = t_data_encoded.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Check for NaN or infinite values in numerical columns before scaling\n",
    "t_nan_inf_columns = t_data_encoded[t_numerical_columns].columns[t_data_encoded[t_numerical_columns].isna().any() | np.isinf(t_data_encoded[t_numerical_columns]).any()].tolist()\n",
    "print(\"Columns with NaN or infinite values before scaling: \", t_nan_inf_columns)\n",
    "\n",
    "def match_columns(new_data, train_encoded_columns, original_dtypes):\n",
    "    missing_cols = {col: 0 for col in train_encoded_columns if col not in new_data.columns}\n",
    "    extra_cols = [col for col in new_data.columns if col not in train_encoded_columns]\n",
    "    \n",
    "    # Add missing columns with 0s\n",
    "    new_data = pd.concat([new_data, pd.DataFrame(missing_cols, index=new_data.index)], axis=1)\n",
    "    \n",
    "    # Remove extra columns\n",
    "    new_data = new_data.drop(columns=extra_cols)\n",
    "    \n",
    "    # Ensure columns are in the same order as training data\n",
    "    new_data = new_data[train_encoded_columns]\n",
    "    \n",
    "    # Convert columns back to their original type\n",
    "    for col, original_dtype in original_dtypes.items():\n",
    "        if col in new_data.columns:  # Ensure the column exists in new_data\n",
    "            if pd.api.types.is_categorical_dtype(original_dtype):\n",
    "                new_data[col] = pd.Categorical(new_data[col])\n",
    "            else:\n",
    "                new_data[col] = new_data[col].astype(original_dtype)\n",
    "            \n",
    "    \n",
    "    return new_data\n",
    "\n",
    "\n",
    "# Apply the function\n",
    "t_data_matched = match_columns(t_data_encoded, train_encoded_columns, original_dtypes)\n",
    "\n",
    "# Check if all columns in t_data_matched are equal to train_encoded_columns\n",
    "if list(t_data_matched.columns) == list(train_encoded_columns):\n",
    "\n",
    "    print(\"All columns in t_data_matched are equal to train_encoded_columns.\")\n",
    "else:\n",
    "    print(\"Columns in t_data_matched and train_encoded_columns do not match.\")\n",
    "\n",
    "    # Check if the columns are the same but in a different order\n",
    "    if set(t_data_matched.columns) == set(train_encoded_columns):\n",
    "        print(\"The datasets have the same columns, but they are in a different order.\")\n",
    "        \n",
    "        # You can reorder the columns in t_data_matched to match train_encoded_columns\n",
    "        t_data_matched = t_data_matched[train_encoded_columns]\n",
    "        print(\"Columns in t_data_matched reordered to match train_encoded_columns.\")\n",
    "    else:\n",
    "        # Identify and print the mismatched columns\n",
    "        extra_cols = set(t_data_matched.columns) - set(train_encoded_columns)\n",
    "        missing_cols = set(train_encoded_columns) - set(t_data_matched.columns)\n",
    "        print(\"Extra columns in t_data_matched: \", extra_cols)\n",
    "        print(\"Missing columns in t_data_matched: \", missing_cols)\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Load the saved scaler\n",
    "scaler = joblib.load(your_path + 'robust_scaler.pkl')\n",
    "\n",
    "# Scale the numerical columns of t_data_matched using the 'numerical_columns' from the training data\n",
    "t_data_matched[numerical_columns] = scaler.transform(t_data_matched[numerical_columns])\n",
    "\n",
    "t_data_matched.to_csv(your_path + test_data_name + '_Scaled.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
