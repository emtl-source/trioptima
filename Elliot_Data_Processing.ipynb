{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process CLEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with a single unique value:  ['assetClass', 'blockTradeIndicator', 'customBasketIndicator', 'deliveryType', 'disseminationTimestamp', 'event', 'instrumentType', 'leg1NotionalScheduleType', 'leg2NotionalScheduleType', 'nonStandardTermIndicator', 'packageIndicator', 'postPricedSwapIndicator', 'primeBrokerageTransactionIndicator']\n",
      "Unique value in assetClass: IR\n",
      "Unique value in blockTradeIndicator: nan\n",
      "Unique value in customBasketIndicator: False\n",
      "Unique value in deliveryType: CASH\n",
      "Unique value in disseminationTimestamp: disseminationTimestamp\n",
      "Unique value in event: TRAD\n",
      "Unique value in instrumentType: Swap\n",
      "Unique value in leg1NotionalScheduleType: Constant\n",
      "Unique value in leg2NotionalScheduleType: Constant\n",
      "Unique value in nonStandardTermIndicator: nan\n",
      "Unique value in packageIndicator: False\n",
      "Unique value in postPricedSwapIndicator: False\n",
      "Unique value in primeBrokerageTransactionIndicator: nan\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/elliotlindestam/Documents/Skola/Indek icloud/trioptima/CLEAN.csv')\n",
    "# Identify columns with only a single unique value\n",
    "single_value_columns = data.columns[data.nunique() == 1].tolist()\n",
    "print(\"Columns with a single unique value: \", single_value_columns)\n",
    "for col in single_value_columns:\n",
    "    print(f\"Unique value in {col}: {data[col].unique()[0]}\")\n",
    "\n",
    "data = data.drop(single_value_columns, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROCESS CLEAN DATA AND SAVE THE SCALER AS robust_scaler.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with a single unique value:  ['customBasketIndicator', 'packageIndicator', 'postPricedSwapIndicator', 'effectiveDate_hour', 'effectiveDate_minute', 'effectiveDate_second', 'expirationDate_hour', 'expirationDate_minute', 'expirationDate_second']\n",
      "Columns with NaN or infinite values before scaling:  []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib\n",
    "\n",
    "your_path = '/Users/elliotlindestam/Documents/Skola/Indek icloud/trioptima/'\n",
    "training_data_name = 'CLEAN'\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('/Users/elliotlindestam/Documents/Skola/Indek icloud/trioptima/CLEAN.csv')\n",
    "\n",
    "# Drop 'disseminationTimestamp' and its derived columns\n",
    "data = data.drop(columns=['disseminationTimestamp','sDRreceiptTimestamp'])\n",
    "\n",
    "# Step 1: Handle Missing Data\n",
    "data_filled = data.fillna(0)\n",
    "\n",
    "# Step 2: Extract Information from DateTime Columns\n",
    "datetime_columns = ['effectiveDate', 'eventDateTime', 'executionDateTime', 'expirationDate']\n",
    "\n",
    "def extract_date_features(df, column):\n",
    "    df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "    df[column + '_year'] = df[column].dt.year\n",
    "    df[column + '_month'] = df[column].dt.month\n",
    "    df[column + '_day'] = df[column].dt.day\n",
    "    df[column + '_hour'] = df[column].dt.hour\n",
    "    df[column + '_minute'] = df[column].dt.minute\n",
    "    df[column + '_second'] = df[column].dt.second\n",
    "    df[column + '_weekday'] = df[column].dt.weekday\n",
    "    # Drop the original datetime column\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df\n",
    "\n",
    "for col in datetime_columns:\n",
    "    data_filled = extract_date_features(data_filled, col)\n",
    "\n",
    "original_dtypes = data.dtypes.to_dict()\n",
    "\n",
    "# Convert boolean columns to binary (1/0) before one-hot encoding other categorical columns\n",
    "data_filled = data_filled*1\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Encode Categorical Variables\n",
    "categorical_columns = data_filled.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "categorical_columns = [col for col in categorical_columns if col not in datetime_columns]\n",
    "data_encoded = pd.get_dummies(data_filled, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Step 3.5: Identify and Drop Single-Value Columns\n",
    "single_value_columns = data_encoded.columns[data_encoded.nunique() == 1].tolist()\n",
    "\n",
    "# Optionally print these columns and their unique values\n",
    "print(\"Columns with a single unique value: \", single_value_columns)\n",
    "\n",
    "# Drop the single-value columns\n",
    "data_encoded = data_encoded.drop(columns=single_value_columns)\n",
    "\n",
    "# Define numerical_columns here, after all column dropping and adding has occurred\n",
    "numerical_columns = data_encoded.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Check for NaN or infinite values in numerical columns before scaling\n",
    "nan_inf_columns = data_encoded[numerical_columns].columns[data_encoded[numerical_columns].isna().any() | np.isinf(data_encoded[numerical_columns]).any()].tolist()\n",
    "print(\"Columns with NaN or infinite values before scaling: \", nan_inf_columns)\n",
    "\n",
    "# Step 4: Scale/Normalize Data\n",
    "numerical_columns = data_encoded.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "scaler = RobustScaler()\n",
    "data_encoded[numerical_columns] = scaler.fit_transform(data_encoded[numerical_columns])\n",
    "\n",
    "# Save the scaler for future use\n",
    "scaler_filename = 'robust_scaler.pkl'\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "# Your preprocessed data is now stored in `data_encoded`, and your scaler is saved as `robust_scaler.pkl`\n",
    "\n",
    "# Save the processed data to a CSV file in this environment\n",
    "data_encoded.to_csv('/Users/elliotlindestam/Documents/Skola/Indek icloud/trioptima/CLEAN_Scaled.csv', index=False)\n",
    "\n",
    "train_encoded_columns = data_encoded.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with a single unique value:  ['blockTradeIndicator', 'customBasketIndicator', 'nonStandardTermIndicator', 'packageIndicator', 'platformID', 'postPricedSwapIndicator', 'primeBrokerageTransactionIndicator', 'effectiveDate_hour', 'effectiveDate_minute', 'effectiveDate_second', 'eventDateTime_year', 'eventDateTime_month', 'eventDateTime_day', 'eventDateTime_weekday', 'executionDateTime_year', 'executionDateTime_month', 'executionDateTime_day', 'executionDateTime_weekday', 'expirationDate_hour', 'expirationDate_minute', 'expirationDate_second']\n",
      "Columns with NaN or infinite values before scaling:  []\n",
      "leg1NotionalAmount: intended dtype: float64, converted dtype: float64\n",
      "leg2NotionalAmount: intended dtype: float64, converted dtype: float64\n",
      "leg2ResetFrequencyMultiplier: intended dtype: float64, converted dtype: float64\n",
      "leg1FixedRate: intended dtype: float64, converted dtype: float64\n",
      "leg1FixedRatePaymentFrequencyMultiplier: intended dtype: float64, converted dtype: float64\n",
      "leg2FloatingRatePaymentFrequencyMultiplier: intended dtype: float64, converted dtype: float64\n",
      "leg2UnderlierTenorMultiplier: intended dtype: float64, converted dtype: float64\n",
      "All columns in t_data_matched are equal to train_encoded_columns.\n",
      "['leg1NotionalAmount', 'leg2NotionalAmount', 'leg2ResetFrequencyMultiplier', 'leg1FixedRate', 'leg1FixedRatePaymentFrequencyMultiplier', 'leg2FloatingRatePaymentFrequencyMultiplier', 'leg2UnderlierTenorMultiplier']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/30rpw85n53d2zxnt_c5qnrx40000gn/T/ipykernel_40751/4098890374.py:74: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(original_dtype):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib\n",
    "\n",
    "# Load the data\n",
    "t_data = pd.read_csv('/Users/elliotlindestam/Documents/Skola/Indek icloud/trioptima/test_Data.csv')\n",
    "\n",
    "# Drop 'disseminationTimestamp' and its derived columns\n",
    "t_data = t_data.drop(columns=['disseminationTimestamp','sDRreceiptTimestamp'])\n",
    "\n",
    "# Step 1: Handle Missing Data\n",
    "t_data_filled = t_data.fillna(0)\n",
    "\n",
    "# Step 2: Extract Information from DateTime Columns\n",
    "t_datetime_columns = ['effectiveDate', 'eventDateTime', 'executionDateTime', 'expirationDate']\n",
    "\n",
    "def extract_date_features(df, column):\n",
    "    df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "    df[column + '_year'] = df[column].dt.year\n",
    "    df[column + '_month'] = df[column].dt.month\n",
    "    df[column + '_day'] = df[column].dt.day\n",
    "    df[column + '_hour'] = df[column].dt.hour\n",
    "    df[column + '_minute'] = df[column].dt.minute\n",
    "    df[column + '_second'] = df[column].dt.second\n",
    "    df[column + '_weekday'] = df[column].dt.weekday\n",
    "    # Drop the original datetime column\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df\n",
    "\n",
    "for col in t_datetime_columns:\n",
    "    t_data_filled = extract_date_features(t_data_filled, col)\n",
    "\n",
    "# Convert boolean columns to binary (1/0) before one-hot encoding other categorical columns\n",
    "t_data_filled = t_data_filled*1\n",
    "\n",
    "# Step 3: Encode Categorical Variables\n",
    "t_categorical_columns = t_data_filled.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "t_categorical_columns = [col for col in t_categorical_columns if col not in t_datetime_columns]\n",
    "t_data_encoded = pd.get_dummies(t_data_filled, columns=t_categorical_columns, drop_first=True)\n",
    "\n",
    "# Step 3.5: Identify and Drop Single-Value Columns\n",
    "t_single_value_columns = t_data_encoded.columns[t_data_encoded.nunique() == 1].tolist()\n",
    "\n",
    "# Optionally print these columns and their unique values\n",
    "print(\"Columns with a single unique value: \", t_single_value_columns)\n",
    "\n",
    "# Drop the single-value columns\n",
    "t_data_encoded = t_data_encoded.drop(columns=t_single_value_columns)\n",
    "\n",
    "# Define numerical_columns here, after all column dropping and adding has occurred\n",
    "t_numerical_columns = t_data_encoded.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Check for NaN or infinite values in numerical columns before scaling\n",
    "t_nan_inf_columns = t_data_encoded[t_numerical_columns].columns[t_data_encoded[t_numerical_columns].isna().any() | np.isinf(t_data_encoded[t_numerical_columns]).any()].tolist()\n",
    "print(\"Columns with NaN or infinite values before scaling: \", t_nan_inf_columns)\n",
    "\n",
    "def match_columns(new_data, train_encoded_columns, original_dtypes):\n",
    "    missing_cols = {col: 0 for col in train_encoded_columns if col not in new_data.columns}\n",
    "    extra_cols = [col for col in new_data.columns if col not in train_encoded_columns]\n",
    "    \n",
    "    # Add missing columns with 0s\n",
    "    new_data = pd.concat([new_data, pd.DataFrame(missing_cols, index=new_data.index)], axis=1)\n",
    "    \n",
    "    # Remove extra columns\n",
    "    new_data = new_data.drop(columns=extra_cols)\n",
    "    \n",
    "    # Ensure columns are in the same order as training data\n",
    "    new_data = new_data[train_encoded_columns]\n",
    "    \n",
    "    # Convert columns back to their original type\n",
    "    for col, original_dtype in original_dtypes.items():\n",
    "        if col in new_data.columns:  # Ensure the column exists in new_data\n",
    "            if pd.api.types.is_categorical_dtype(original_dtype):\n",
    "                new_data[col] = pd.Categorical(new_data[col])\n",
    "            else:\n",
    "                new_data[col] = new_data[col].astype(original_dtype)\n",
    "            \n",
    "            # Debug: Print the column, its intended dtype, and the resulting dtype\n",
    "            print(f\"{col}: intended dtype: {original_dtype}, converted dtype: {new_data[col].dtype}\")\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "\n",
    "\n",
    "# Apply the function\n",
    "t_data_matched = match_columns(t_data_encoded, train_encoded_columns, original_dtypes)\n",
    "\n",
    "# Check if all columns in t_data_matched are equal to train_encoded_columns\n",
    "if list(t_data_matched.columns) == list(train_encoded_columns):\n",
    "\n",
    "    print(\"All columns in t_data_matched are equal to train_encoded_columns.\")\n",
    "else:\n",
    "    print(\"Columns in t_data_matched and train_encoded_columns do not match.\")\n",
    "\n",
    "    # Check if the columns are the same but in a different order\n",
    "    if set(t_data_matched.columns) == set(train_encoded_columns):\n",
    "        print(\"The datasets have the same columns, but they are in a different order.\")\n",
    "        \n",
    "        # You can reorder the columns in t_data_matched to match train_encoded_columns\n",
    "        t_data_matched = t_data_matched[train_encoded_columns]\n",
    "        print(\"Columns in t_data_matched reordered to match train_encoded_columns.\")\n",
    "    else:\n",
    "        # Identify and print the mismatched columns\n",
    "        extra_cols = set(t_data_matched.columns) - set(train_encoded_columns)\n",
    "        missing_cols = set(train_encoded_columns) - set(t_data_matched.columns)\n",
    "        print(\"Extra columns in t_data_matched: \", extra_cols)\n",
    "        print(\"Missing columns in t_data_matched: \", missing_cols)\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Load the saved scaler\n",
    "scaler = joblib.load('/Users/elliotlindestam/Documents/Skola/Indek icloud/trioptima/robust_scaler.pkl')\n",
    "print(numerical_columns)\n",
    "# Scale the numerical columns of t_data_matched using the 'numerical_columns' from the training data\n",
    "t_data_matched[t_numerical_columns] = scaler.transform(t_data_matched[t_numerical_columns])\n",
    "\n",
    "t_data_encoded.to_csv('/Users/elliotlindestam/Documents/Skola/Indek icloud/trioptima/test_Scaled.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
