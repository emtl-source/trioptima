{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE DATA PROCESSING, CURRENTLY SET TO PROCESS TO FILES - ONE TRAINING FILE FOR THE AUTOENCODER MODEL AND ONE TRADE FILE WHERE OUTLIERS CAN BE TESTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Columns: ['leg1NotionalAmount', 'leg2NotionalAmount', 'leg2ResetFrequencyMultiplier', 'leg1FixedRate', 'leg1FixedRatePaymentFrequencyMultiplier', 'leg2FloatingRatePaymentFrequencyMultiplier', 'leg2UnderlierTenorMultiplier']\n",
      "Categorical Columns: ['action', 'assetClass', 'blockTradeIndicator', 'cleared', 'deliveryType', 'disseminationTimestamp', 'event', 'instrumentType', 'leg1FixedRateDayCount', 'leg1FloatingRateDayCount', 'leg1NotionalCurrency', 'leg1NotionalScheduleType', 'leg1SettlementCurrency', 'leg1UnderlyingAssetOrContractType', 'leg2FloatingRateDayCount', 'leg2NotionalCurrency', 'leg2NotionalScheduleType', 'leg2SettlementCurrency', 'nonStandardTermIndicator', 'platformID', 'primeBrokerageTransactionIndicator', 'productName', 'leg1FixedRatePaymentFrequencyPeriod', 'leg2FloatingRatePaymentFrequencyPeriod', 'leg2UnderlierCurrency', 'leg2UnderlierID', 'leg2UnderlierIDSource', 'leg2UnderlierTenorPeriod', 'leg2ResetFrequencyPeriod']\n",
      "Data Shape after Preprocessing: (5804, 504)\n",
      "Number of Feature Names: 504\n",
      "Numerical Columns: ['blockTradeIndicator', 'leg1NotionalAmount', 'leg2NotionalAmount', 'leg2ResetFrequencyMultiplier', 'nonStandardTermIndicator', 'platformID', 'primeBrokerageTransactionIndicator', 'leg1FixedRate', 'leg1FixedRatePaymentFrequencyMultiplier', 'leg2FloatingRatePaymentFrequencyMultiplier', 'leg2UnderlierTenorMultiplier']\n",
      "Categorical Columns: ['action', 'assetClass', 'cleared', 'deliveryType', 'disseminationTimestamp', 'event', 'instrumentType', 'leg1FixedRateDayCount', 'leg1FloatingRateDayCount', 'leg1NotionalCurrency', 'leg1NotionalScheduleType', 'leg1SettlementCurrency', 'leg1UnderlyingAssetOrContractType', 'leg2FloatingRateDayCount', 'leg2NotionalCurrency', 'leg2NotionalScheduleType', 'leg2SettlementCurrency', 'productName', 'leg1FixedRatePaymentFrequencyPeriod', 'leg2FloatingRatePaymentFrequencyPeriod', 'leg2UnderlierCurrency', 'leg2UnderlierID', 'leg2UnderlierIDSource', 'leg2UnderlierTenorPeriod', 'leg2ResetFrequencyPeriod']\n",
      "Data Shape after Preprocessing: (20, 504)\n",
      "Number of Feature Names: 504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gusta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\impute\\_base.py:558: UserWarning: Skipping features without any observed values: ['blockTradeIndicator' 'nonStandardTermIndicator' 'platformID'\n",
      " 'primeBrokerageTransactionIndicator']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def process_data(data, date_cols, output_path, train_feature_names=None):\n",
    "    \"\"\"\n",
    "    Preprocesses the input data and writes the result to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): Input data to be preprocessed.\n",
    "    date_cols (list): List of date columns to be transformed.\n",
    "    output_path (str): Path for the processed data CSV file.\n",
    "    train_feature_names (list): Feature names from the training data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The preprocessed data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Handling DateTime Variables\n",
    "    for col in date_cols:\n",
    "        data[col] = pd.to_datetime(data[col], errors='coerce')  # Convert to datetime\n",
    "        data[f'{col}Year'] = data[col].dt.year  # Extract year\n",
    "        data[f'{col}Month'] = data[col].dt.month  # Extract month\n",
    "        data[f'{col}Day'] = data[col].dt.day  # Extract day\n",
    "        data = data.drop(columns=[col])  # Drop the original date column\n",
    "\n",
    "    # Automatically identify numerical and categorical variables\n",
    "    numerical_vars = [col for col in data.select_dtypes(include=[np.number]).columns.tolist() \n",
    "                      if not (col.endswith('Year') or col.endswith('Month') or col.endswith('Day'))]\n",
    "    categorical_vars = data.select_dtypes(include=[object]).columns.tolist()\n",
    "\n",
    "    # Print information for verification\n",
    "    print(f\"Numerical Columns: {numerical_vars}\")\n",
    "    print(f\"Categorical Columns: {categorical_vars}\")\n",
    "\n",
    "    # Create transformers\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Initialize ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical_vars),\n",
    "            ('cat', categorical_transformer, categorical_vars)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Fit and transform the data\n",
    "    data_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "    # Get feature names after one-hot encoding\n",
    "    onehot_columns = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_vars)\n",
    "\n",
    "    # Combine the feature names\n",
    "    feature_names = numerical_vars + onehot_columns.tolist()\n",
    "\n",
    "    # If the function was provided training feature names, add any missing columns with zeros\n",
    "    if train_feature_names is not None:\n",
    "        missing_columns = set(train_feature_names) - set(feature_names)\n",
    "        for missing_col in missing_columns:\n",
    "            data_preprocessed = np.column_stack((data_preprocessed, np.zeros(data_preprocessed.shape[0])))\n",
    "        feature_names = train_feature_names\n",
    "\n",
    "    # Print and verify the lengths\n",
    "    print(f\"Data Shape after Preprocessing: {data_preprocessed.shape}\")\n",
    "    print(f\"Number of Feature Names: {len(feature_names)}\")\n",
    "\n",
    "    data_preprocessed_dense = data_preprocessed.toarray() if hasattr(data_preprocessed, 'toarray') else data_preprocessed\n",
    "    data_preprocessed_df = pd.DataFrame(data_preprocessed_dense, columns=feature_names)\n",
    "\n",
    "    # Handle columns with all missing values\n",
    "    for col in data_preprocessed_df.columns:\n",
    "        if data_preprocessed_df[col].isnull().all():\n",
    "            data_preprocessed_df[col].fillna(0, inplace=True)  # fill NaN with 0 for these particular columns\n",
    "\n",
    "    # Save preprocessed data to CSV\n",
    "    data_preprocessed_df.to_csv(output_path, index=False)\n",
    "    return data_preprocessed_df\n",
    "\n",
    "# SET THIS TO THE FOLDER WHERE THE FILES ARE LOCATED\n",
    "your_path = r'C:\\Users\\gusta\\Documents\\KTH\\TriOptima\\trioptima'\n",
    "\n",
    "# For training data\n",
    "file_name1 = '\\CLEAN'\n",
    "file_path1 = your_path + file_name1 + '.csv'\n",
    "date_cols = ['effectiveDate', 'eventDateTime', 'executionDateTime', 'expirationDate', 'sDRreceiptTimestamp']\n",
    "data1 = pd.read_csv(file_path1)\n",
    "output_path1 = your_path + file_name1 + '_Processed.csv'\n",
    "processed_data_clean = process_data(data1, date_cols, output_path1)\n",
    "\n",
    "# For trade data\n",
    "file_name2 = '\\TRADES'\n",
    "file_path2 = your_path + file_name2 + '.csv'\n",
    "data2 = pd.read_csv(file_path2)\n",
    "output_path2 = your_path + file_name2 + '_Processed.csv'\n",
    "processed_data_trade = process_data(data2, date_cols, output_path2, processed_data_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW ATTEMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "input_features should have length equal to number of features (29), got 25",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gusta\\Documents\\KTH\\TriOptima\\trioptima\\USE_THIS_Data_Processing.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gusta/Documents/KTH/TriOptima/trioptima/USE_THIS_Data_Processing.ipynb#W2sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m data2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(file_path2)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gusta/Documents/KTH/TriOptima/trioptima/USE_THIS_Data_Processing.ipynb#W2sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m output_path2 \u001b[39m=\u001b[39m your_path \u001b[39m+\u001b[39m file_name2 \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_Processed.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/gusta/Documents/KTH/TriOptima/trioptima/USE_THIS_Data_Processing.ipynb#W2sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m processed_data_trade \u001b[39m=\u001b[39m transform_data(data2, date_cols, output_path2, preprocessor_path)\n",
      "\u001b[1;32mc:\\Users\\gusta\\Documents\\KTH\\TriOptima\\trioptima\\USE_THIS_Data_Processing.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gusta/Documents/KTH/TriOptima/trioptima/USE_THIS_Data_Processing.ipynb#W2sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m preprocessor \u001b[39m=\u001b[39m load(preprocessor_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gusta/Documents/KTH/TriOptima/trioptima/USE_THIS_Data_Processing.ipynb#W2sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m data_preprocessed \u001b[39m=\u001b[39m preprocessor\u001b[39m.\u001b[39mtransform(data)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gusta/Documents/KTH/TriOptima/trioptima/USE_THIS_Data_Processing.ipynb#W2sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m onehot_columns \u001b[39m=\u001b[39m preprocessor\u001b[39m.\u001b[39;49mnamed_transformers_[\u001b[39m'\u001b[39;49m\u001b[39mcat\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mnamed_steps[\u001b[39m'\u001b[39;49m\u001b[39monehot\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mget_feature_names_out(data\u001b[39m.\u001b[39;49mselect_dtypes(include\u001b[39m=\u001b[39;49m[\u001b[39mobject\u001b[39;49m])\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mtolist())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gusta/Documents/KTH/TriOptima/trioptima/USE_THIS_Data_Processing.ipynb#W2sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m feature_names \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mselect_dtypes(include\u001b[39m=\u001b[39m[np\u001b[39m.\u001b[39mnumber])\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mtolist() \u001b[39m+\u001b[39m onehot_columns\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gusta/Documents/KTH/TriOptima/trioptima/USE_THIS_Data_Processing.ipynb#W2sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m data_preprocessed_dense \u001b[39m=\u001b[39m data_preprocessed\u001b[39m.\u001b[39mtoarray() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(data_preprocessed, \u001b[39m'\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m data_preprocessed\n",
      "File \u001b[1;32mc:\\Users\\gusta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1211\u001b[0m, in \u001b[0;36mOneHotEncoder.get_feature_names_out\u001b[1;34m(self, input_features)\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get output feature names for transformation.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \n\u001b[0;32m   1193\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1208\u001b[0m \u001b[39m    Transformed feature names.\u001b[39;00m\n\u001b[0;32m   1209\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1210\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m-> 1211\u001b[0m input_features \u001b[39m=\u001b[39m _check_feature_names_in(\u001b[39mself\u001b[39;49m, input_features)\n\u001b[0;32m   1212\u001b[0m cats \u001b[39m=\u001b[39m [\n\u001b[0;32m   1213\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_transformed_categories(i)\n\u001b[0;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m i, _ \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories_)\n\u001b[0;32m   1215\u001b[0m ]\n\u001b[0;32m   1217\u001b[0m name_combiner \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_get_feature_name_combiner()\n",
      "File \u001b[1;32mc:\\Users\\gusta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2074\u001b[0m, in \u001b[0;36m_check_feature_names_in\u001b[1;34m(estimator, input_features, generate_names)\u001b[0m\n\u001b[0;32m   2071\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39minput_features is not equal to feature_names_in_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2073\u001b[0m     \u001b[39mif\u001b[39;00m n_features_in_ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(input_features) \u001b[39m!=\u001b[39m n_features_in_:\n\u001b[1;32m-> 2074\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2075\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minput_features should have length equal to number of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2076\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfeatures (\u001b[39m\u001b[39m{\u001b[39;00mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m), got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(input_features)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2077\u001b[0m         )\n\u001b[0;32m   2078\u001b[0m     \u001b[39mreturn\u001b[39;00m input_features\n\u001b[0;32m   2080\u001b[0m \u001b[39mif\u001b[39;00m feature_names_in_ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: input_features should have length equal to number of features (29), got 25"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from joblib import dump, load\n",
    "\n",
    "def fit_transform_data(data, date_cols, output_path, preprocessor_path):\n",
    "    \"\"\"\n",
    "    Fits transformers, preprocesses the input data, and writes the result to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): Input data to be preprocessed.\n",
    "    date_cols (list): List of date columns to be transformed.\n",
    "    output_path (str): Path for the processed data CSV file.\n",
    "    preprocessor_path (str): Path to save the fitted preprocessor.\n",
    "    \"\"\"\n",
    "\n",
    "    # Handling DateTime Variables\n",
    "    for col in date_cols:\n",
    "        data[col] = pd.to_datetime(data[col], errors='coerce')\n",
    "        data[f'{col}Year'] = data[col].dt.year\n",
    "        data[f'{col}Month'] = data[col].dt.month\n",
    "        data[f'{col}Day'] = data[col].dt.day\n",
    "        data = data.drop(columns=[col])\n",
    "\n",
    "    # Identify numerical and categorical variables\n",
    "    numerical_vars = [col for col in data.select_dtypes(include=[np.number]).columns.tolist() if not col.endswith(('Year', 'Month', 'Day'))]\n",
    "    categorical_vars = data.select_dtypes(include=[object]).columns.tolist()\n",
    "\n",
    "    # Create transformers\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Initialize and fit ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical_vars),\n",
    "            ('cat', categorical_transformer, categorical_vars)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    data_preprocessed = preprocessor.fit_transform(data)\n",
    "    dump(preprocessor, preprocessor_path)\n",
    "\n",
    "    # Get feature names after one-hot encoding\n",
    "    onehot_columns = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_vars)\n",
    "    feature_names = numerical_vars + onehot_columns.tolist()\n",
    "\n",
    "    data_preprocessed_dense = data_preprocessed.toarray() if hasattr(data_preprocessed, 'toarray') else data_preprocessed\n",
    "    data_preprocessed_df = pd.DataFrame(data_preprocessed_dense, columns=feature_names)\n",
    "\n",
    "    data_preprocessed_df.to_csv(output_path, index=False)\n",
    "    return data_preprocessed_df\n",
    "\n",
    "def transform_data(data, date_cols, output_path, preprocessor_path):\n",
    "    \"\"\"\n",
    "    Uses a fitted preprocessor to transform the input data and writes the result to a CSV file.\n",
    "    \"\"\"\n",
    "    for col in date_cols:\n",
    "        data[col] = pd.to_datetime(data[col], errors='coerce')\n",
    "        data[f'{col}Year'] = data[col].dt.year\n",
    "        data[f'{col}Month'] = data[col].dt.month\n",
    "        data[f'{col}Day'] = data[col].dt.day\n",
    "        data = data.drop(columns=[col])\n",
    "\n",
    "    preprocessor = load(preprocessor_path)\n",
    "    data_preprocessed = preprocessor.transform(data)\n",
    "\n",
    "    onehot_columns = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(data.select_dtypes(include=[object]).columns.tolist())\n",
    "    feature_names = data.select_dtypes(include=[np.number]).columns.tolist() + onehot_columns.tolist()\n",
    "\n",
    "    data_preprocessed_dense = data_preprocessed.toarray() if hasattr(data_preprocessed, 'toarray') else data_preprocessed\n",
    "    data_preprocessed_df = pd.DataFrame(data_preprocessed_dense, columns=feature_names)\n",
    "\n",
    "    data_preprocessed_df.to_csv(output_path, index=False)\n",
    "    return data_preprocessed_df\n",
    "\n",
    "# Example Usage\n",
    "your_path = r'C:\\Users\\gusta\\Documents\\KTH\\TriOptima\\trioptima'\n",
    "\n",
    "# For training data\n",
    "file_name1 = '\\CLEAN'\n",
    "file_path1 = your_path + file_name1 + '.csv'\n",
    "date_cols = ['effectiveDate', 'eventDateTime', 'executionDateTime', 'expirationDate', 'sDRreceiptTimestamp']\n",
    "data1 = pd.read_csv(file_path1)\n",
    "output_path1 = your_path + file_name1 + '_Processed.csv'\n",
    "preprocessor_path = your_path + '/fitted_preprocessor.joblib'\n",
    "\n",
    "processed_data_clean = fit_transform_data(data1, date_cols, output_path1, preprocessor_path)\n",
    "\n",
    "# For trade data\n",
    "file_name2 = '\\TRADES'\n",
    "file_path2 = your_path + file_name2 + '.csv'\n",
    "data2 = pd.read_csv(file_path2)\n",
    "output_path2 = your_path + file_name2 + '_Processed.csv'\n",
    "\n",
    "processed_data_trade = transform_data(data2, date_cols, output_path2, preprocessor_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Columns: ['leg1NotionalAmount', 'leg2NotionalAmount', 'leg2ResetFrequencyMultiplier', 'leg1FixedRate', 'leg1FixedRatePaymentFrequencyMultiplier', 'leg2FloatingRatePaymentFrequencyMultiplier', 'leg2UnderlierTenorMultiplier']\n",
      "Categorical Columns: ['action', 'assetClass', 'blockTradeIndicator', 'cleared', 'deliveryType', 'disseminationTimestamp', 'event', 'instrumentType', 'leg1FixedRateDayCount', 'leg1FloatingRateDayCount', 'leg1NotionalCurrency', 'leg1NotionalScheduleType', 'leg1SettlementCurrency', 'leg1UnderlyingAssetOrContractType', 'leg2FloatingRateDayCount', 'leg2NotionalCurrency', 'leg2NotionalScheduleType', 'leg2SettlementCurrency', 'nonStandardTermIndicator', 'platformID', 'primeBrokerageTransactionIndicator', 'productName', 'leg1FixedRatePaymentFrequencyPeriod', 'leg2FloatingRatePaymentFrequencyPeriod', 'leg2UnderlierCurrency', 'leg2UnderlierID', 'leg2UnderlierIDSource', 'leg2UnderlierTenorPeriod', 'leg2ResetFrequencyPeriod']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gusta\\Documents\\KTH\\TriOptima\\trioptima\\USE_THIS_Data_Processing.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gusta/Documents/KTH/TriOptima/trioptima/USE_THIS_Data_Processing.ipynb#W5sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m output_path1 \u001b[39m=\u001b[39m your_path \u001b[39m+\u001b[39m file_name1 \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_Processed.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gusta/Documents/KTH/TriOptima/trioptima/USE_THIS_Data_Processing.ipynb#W5sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m preprocessor_path \u001b[39m=\u001b[39m your_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mpreprocessor.joblib\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/gusta/Documents/KTH/TriOptima/trioptima/USE_THIS_Data_Processing.ipynb#W5sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m processed_data_clean \u001b[39m=\u001b[39m process_data(data1, date_cols, output_path1, preprocessor_path)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gusta/Documents/KTH/TriOptima/trioptima/USE_THIS_Data_Processing.ipynb#W5sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39m# For trade data\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gusta/Documents/KTH/TriOptima/trioptima/USE_THIS_Data_Processing.ipynb#W5sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m file_name2 \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mTRADES\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;32mc:\\Users\\gusta\\Documents\\KTH\\TriOptima\\trioptima\\USE_THIS_Data_Processing.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gusta/Documents/KTH/TriOptima/trioptima/USE_THIS_Data_Processing.ipynb#W5sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m preprocessor \u001b[39m=\u001b[39m load(preprocessor_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gusta/Documents/KTH/TriOptima/trioptima/USE_THIS_Data_Processing.ipynb#W5sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Ensure the data has the same columns as the data the preprocessor was trained on\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gusta/Documents/KTH/TriOptima/trioptima/USE_THIS_Data_Processing.ipynb#W5sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m expected_numeric_columns \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(preprocessor\u001b[39m.\u001b[39;49mnamed_transformers_[\u001b[39m'\u001b[39;49m\u001b[39mnum\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mnamed_steps[\u001b[39m'\u001b[39;49m\u001b[39mscaler\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mmean_\u001b[39m.\u001b[39;49mindex)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gusta/Documents/KTH/TriOptima/trioptima/USE_THIS_Data_Processing.ipynb#W5sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m expected_categorical_columns \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(preprocessor\u001b[39m.\u001b[39mnamed_transformers_[\u001b[39m'\u001b[39m\u001b[39mcat\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mnamed_steps[\u001b[39m'\u001b[39m\u001b[39monehot\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mget_feature_names_out(categorical_vars))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gusta/Documents/KTH/TriOptima/trioptima/USE_THIS_Data_Processing.ipynb#W5sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m expected_columns \u001b[39m=\u001b[39m expected_numeric_columns\u001b[39m.\u001b[39munion(expected_categorical_columns)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from joblib import dump, load\n",
    "\n",
    "def process_data(data, date_cols, output_path, preprocessor_path=None):\n",
    "    \"\"\"\n",
    "    ... (the rest of your documentation) ...\n",
    "    \"\"\"\n",
    "    # Handling DateTime Variables\n",
    "    for col in date_cols:\n",
    "        data[col] = pd.to_datetime(data[col], errors='coerce')\n",
    "        data[f'{col}Year'] = data[col].dt.year\n",
    "        data[f'{col}Month'] = data[col].dt.month\n",
    "        data[f'{col}Day'] = data[col].dt.day\n",
    "        data = data.drop(columns=[col])\n",
    "\n",
    "    # Automatically identify numerical and categorical variables\n",
    "    numerical_vars = [col for col in data.select_dtypes(include=[np.number]).columns.tolist() \n",
    "                      if not (col.endswith('Year') or col.endswith('Month') or col.endswith('Day'))]\n",
    "    categorical_vars = data.select_dtypes(include=[object]).columns.tolist()\n",
    "\n",
    "    # Print information for verification\n",
    "    print(f\"Numerical Columns: {numerical_vars}\")\n",
    "    print(f\"Categorical Columns: {categorical_vars}\")\n",
    "\n",
    "    # Create transformers\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Initialize ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical_vars),\n",
    "            ('cat', categorical_transformer, categorical_vars)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Fit and transform the data\n",
    "    if preprocessor_path is not None and os.path.exists(preprocessor_path):\n",
    "        preprocessor = load(preprocessor_path)\n",
    "        \n",
    "        # Ensure the data has the same columns as the data the preprocessor was trained on\n",
    "        expected_numeric_columns = set(preprocessor.named_transformers_['num'].named_steps['scaler'].mean_.index)\n",
    "        expected_categorical_columns = set(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_vars))\n",
    "        expected_columns = expected_numeric_columns.union(expected_categorical_columns)\n",
    "        \n",
    "        input_numeric_columns = set(data.select_dtypes(include=[np.number]).columns.tolist())\n",
    "        input_categorical_columns = set(data.select_dtypes(include=[object]).columns.tolist())\n",
    "        input_columns = input_numeric_columns.union(input_categorical_columns)\n",
    "\n",
    "        missing_columns = expected_columns - input_columns\n",
    "\n",
    "        if missing_columns:\n",
    "            print(f\"Warning: Missing columns detected:\")\n",
    "            print(f\"Columns expected but not found in input data: {missing_columns}\")\n",
    "            for col in missing_columns:\n",
    "                if col in expected_numeric_columns:\n",
    "                    data[col] = 0  # or use np.nan or another default value\n",
    "                else:\n",
    "                    data[col] = 'missing'  # or another default value\n",
    "        \n",
    "        # Ensure columns are in the expected order\n",
    "        data = data[expected_columns]\n",
    "        data_preprocessed = preprocessor.transform(data)\n",
    "\n",
    "    else:\n",
    "        data_preprocessed = preprocessor.fit_transform(data)\n",
    "        if preprocessor_path is not None:\n",
    "            dump(preprocessor, preprocessor_path)\n",
    "\n",
    "    onehot_columns = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_vars)\n",
    "    feature_names = numerical_vars + onehot_columns.tolist()\n",
    "    print(f\"Data Shape after Preprocessing: {data_preprocessed.shape}\")\n",
    "    print(f\"Number of Feature Names: {len(feature_names)}\")\n",
    "    data_preprocessed_dense = data_preprocessed.toarray() if hasattr(data_preprocessed, 'toarray') else data_preprocessed\n",
    "    data_preprocessed_df = pd.DataFrame(data_preprocessed_dense, columns=feature_names)\n",
    "    data_preprocessed_df.to_csv(output_path, index=False)\n",
    "    return data_preprocessed_df\n",
    "\n",
    "# Replace your_path with your actual path\n",
    "your_path = r'C:\\Users\\gusta\\Documents\\KTH\\TriOptima\\trioptima'\n",
    "\n",
    "# For training data\n",
    "file_name1 = '\\CLEAN'\n",
    "file_path1 = your_path + file_name1 + '.csv'\n",
    "date_cols = ['effectiveDate', 'eventDateTime', 'executionDateTime', 'expirationDate', 'sDRreceiptTimestamp']\n",
    "data1 = pd.read_csv(file_path1)\n",
    "output_path1 = your_path + file_name1 + '_Processed.csv'\n",
    "preprocessor_path = your_path + '\\preprocessor.joblib'\n",
    "processed_data_clean = process_data(data1, date_cols, output_path1, preprocessor_path)\n",
    "\n",
    "# For trade data\n",
    "file_name2 = '\\TRADES'\n",
    "file_path2 = your_path + file_name2 + '.csv'\n",
    "data2 = pd.read_csv(file_path2)\n",
    "output_path2 = your_path + file_name2 + '_Processed.csv'\n",
    "processed_data_trade = process_data(data2, date_cols, output_path2, preprocessor_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
