{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 1 - PROCESS CLEAN DATA (TRAINING) AND SAVE THE SCALER AS robust_scaler.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/30rpw85n53d2zxnt_c5qnrx40000gn/T/ipykernel_88171/822816036.py:18: DtypeWarning: Columns (1,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(your_path + \"3.Cash_Risk/\" + training_data_name+'_Cash_Risk.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with a single unique value:  ['customBasketIndicator', 'packageIndicator', 'postPricedSwapIndicator', 'CurrencyIV', 'NominalIV', 'effectiveDate_hour', 'effectiveDate_minute', 'effectiveDate_second', 'executionDateTime_year', 'executionDateTime_month', 'executionDateTime_day', 'executionDateTime_hour', 'executionDateTime_minute', 'executionDateTime_second', 'executionDateTime_weekday', 'expirationDate_hour', 'expirationDate_minute', 'expirationDate_second']\n",
      "Index(['leg1NotionalAmount', 'leg2NotionalAmount', 'leg1FixedRate',\n",
      "       'leg1FixedRatePaymentFrequencyMultiplier',\n",
      "       'leg2UnderlierTenorMultiplier', 'leg1NotionalAmountUSD', 'MtM_leg1',\n",
      "       'MtM_leg2', 'total_delta', 'effectiveDate_year', 'effectiveDate_month',\n",
      "       'effectiveDate_day', 'effectiveDate_weekday', 'expirationDate_year',\n",
      "       'expirationDate_month', 'expirationDate_day', 'expirationDate_weekday',\n",
      "       'cleared_Y', 'event_Allocation', 'event_Amendment',\n",
      "       'event_BLENDING_REMNANT', 'event_Clearing Novation',\n",
      "       'event_Compression', 'event_DSF', 'event_FULL_NETTING',\n",
      "       'event_Full Netting', 'event_NETTING_REMNANT', 'event_New Trade',\n",
      "       'event_Other', 'event_PARTIAL_NETTING', 'event_Partial Netting',\n",
      "       'event_Partial Swap Unwind', 'event_RT Cancellation',\n",
      "       'event_RT Correction', 'event_Swap Unwind', 'event_TERMINATION_OFFSET',\n",
      "       'event_TRAD', 'event_TRANSFER_IN', 'event_UNDEFINED', 'event_Void',\n",
      "       'leg1FixedRateDayCount_30/360', 'leg1FixedRateDayCount_365/360',\n",
      "       'leg1FixedRateDayCount_A001', 'leg1FixedRateDayCount_A004',\n",
      "       'leg1FixedRateDayCount_A005', 'leg1FixedRateDayCount_ACT/360',\n",
      "       'leg1FixedRateDayCount_ACT/365.FIXED', 'leg1NotionalCurrency_CAD',\n",
      "       'leg1NotionalCurrency_EUR', 'leg1NotionalCurrency_GBP',\n",
      "       'leg1NotionalCurrency_NZD', 'leg1NotionalCurrency_USD',\n",
      "       'leg1SettlementCurrency_CAD', 'leg1SettlementCurrency_EUR',\n",
      "       'leg1SettlementCurrency_GBP', 'leg1SettlementCurrency_NZD',\n",
      "       'leg1SettlementCurrency_USD', 'leg1UnderlyingAssetOrContractType_OIS',\n",
      "       'leg2NotionalCurrency_CAD', 'leg2NotionalCurrency_EUR',\n",
      "       'leg2NotionalCurrency_GBP', 'leg2NotionalCurrency_NZD',\n",
      "       'leg2NotionalCurrency_USD', 'leg2SettlementCurrency_CAD',\n",
      "       'leg2SettlementCurrency_EUR', 'leg2SettlementCurrency_GBP',\n",
      "       'leg2SettlementCurrency_NZD', 'leg2SettlementCurrency_USD',\n",
      "       'leg1FixedRatePaymentFrequencyPeriod_MNTH',\n",
      "       'leg1FixedRatePaymentFrequencyPeriod_YEAR', 'leg2UnderlierID_AUD-BBSW',\n",
      "       'leg2UnderlierID_CAD-CDOR', 'leg2UnderlierID_CAD-CORRA-OIS Compound',\n",
      "       'leg2UnderlierID_EUR-EURIBOR',\n",
      "       'leg2UnderlierID_EUR-EuroSTR-OIS Compound',\n",
      "       'leg2UnderlierID_GBP-SONIA-OIS Compound',\n",
      "       'leg2UnderlierID_NZD-BKBM FRA',\n",
      "       'leg2UnderlierID_NZD-NZIONA-OIS Compound',\n",
      "       'leg2UnderlierID_USD-Federal Funds-H.15-OIS-COMPOUND',\n",
      "       'leg2UnderlierID_USD-LIBOR-BBA', 'leg2UnderlierID_USD-SOFR-1M',\n",
      "       'leg2UnderlierID_USD-SOFR-OIS Compound',\n",
      "       'leg2UnderlierTenorPeriod_YEAR', 'CurrencyIV', 'NominalIV'],\n",
      "      dtype='object')\n",
      "Columns with NaN or infinite values before scaling:  []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "#r'C:/Users/gusta/Documents/KTH/TriOptima/trioptima/trioptima/'\n",
    "#'/Users/elliotlindestam/Documents/Skola/Indek icloud/trioptima/'\n",
    "your_path = '/Users/elliotlindestam/Documents/Skola/Indek icloud/trioptima/'\n",
    "folder_path = your_path + '6.Active Data/Train Model Data/'\n",
    "# Get file in the folder\n",
    "files = os.listdir(folder_path)\n",
    "# MAC issue\n",
    "files = [f for f in files if f != '.DS_Store']\n",
    "training_data_name = files[0][:-4]\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(your_path + \"3.Cash_Risk/\" + training_data_name+'_Cash_Risk.csv')\n",
    "\n",
    "# Step 1: Handle Missing Data\n",
    "data_filled = data.fillna(0)\n",
    "\n",
    "# Step 2: Extract Information from DateTime Columns\n",
    "datetime_columns = ['effectiveDate', 'executionDateTime', 'expirationDate'] # ['effectiveDate', 'expirationDate', 'eventDateTime',] # \n",
    "\n",
    "def extract_date_features(df, column):\n",
    "    df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "    df[column + '_year'] = df[column].dt.year\n",
    "    df[column + '_month'] = df[column].dt.month\n",
    "    df[column + '_day'] = df[column].dt.day\n",
    "    df[column + '_hour'] = df[column].dt.hour\n",
    "    df[column + '_minute'] = df[column].dt.minute\n",
    "    df[column + '_second'] = df[column].dt.second\n",
    "    df[column + '_weekday'] = df[column].dt.weekday\n",
    "    # Drop the original datetime column\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df\n",
    "\n",
    "for col in datetime_columns:\n",
    "    data_filled = extract_date_features(data_filled, col)\n",
    "\n",
    "original_dtypes = data.dtypes.to_dict()\n",
    "\n",
    "# Convert boolean columns to binary (1/0) before one-hot encoding other categorical columns\n",
    "data_filled = data_filled*1\n",
    "\n",
    "def compare_item(row,leg1,leg2):\n",
    "    return 1 if row[leg1] == row[leg2] else 0\n",
    "\n",
    "cur = data_filled.apply(lambda row: compare_item(row, 'leg1NotionalCurrency','leg2NotionalCurrency'), axis=1)\n",
    "nom = data_filled.apply(lambda row: compare_item(row, 'leg1NotionalAmount','leg2NotionalAmount'), axis=1)\n",
    "\n",
    "# Step 3: Encode Categorical Variables\n",
    "categorical_columns = data_filled.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "categorical_columns = [col for col in categorical_columns if col not in datetime_columns]\n",
    "data_encoded = pd.get_dummies(data_filled, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Step 3.5: Identify and Drop Single-Value Columns\n",
    "single_value_columns = data_encoded.columns[data_encoded.nunique() == 1].tolist()\n",
    "\n",
    "# Optionally print these columns and their unique values\n",
    "print(\"Columns with a single unique value: \", single_value_columns)\n",
    "\n",
    "# Drop the single-value columns\n",
    "data_encoded = data_encoded.drop(columns=single_value_columns)\n",
    "data_encoded['CurrencyIV'] = cur\n",
    "data_encoded['NominalIV'] = nom\n",
    "#data_encoded = data_encoded.fillna(0)\n",
    "# Define numerical_columns here, after all column dropping and adding has occurred\n",
    "\n",
    "\n",
    "\n",
    "print(data_encoded.columns)\n",
    "# Create interaction variable\n",
    "\n",
    "\n",
    "numerical_columns = data_encoded.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Check for NaN or infinite values in numerical columns before scaling\n",
    "nan_inf_columns = data_encoded[numerical_columns].columns[data_encoded[numerical_columns].isna().any() | np.isinf(data_encoded[numerical_columns]).any()].tolist()\n",
    "print(\"Columns with NaN or infinite values before scaling: \", nan_inf_columns)\n",
    "\n",
    "# Step 4: Scale/Normalize Data\n",
    "numerical_columns = data_encoded.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "scaler = RobustScaler()\n",
    "data_encoded[numerical_columns] = scaler.fit_transform(data_encoded[numerical_columns])\n",
    "\n",
    "# Save the scaler for future use\n",
    "scaler_filename = 'robust_scaler.pkl'\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "# Preprocessed data is now stored in `data_encoded`, and the scaler is saved as `robust_scaler.pkl`\n",
    "# Save the processed data to a CSV file in this environment\n",
    "data_encoded.to_csv(your_path + '4.Scaled/' + training_data_name + '_Scaled.csv', index=False)\n",
    "\n",
    "train_encoded_columns = data_encoded.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2 - PROCESS THE TEST DATA USING THE SCALER IN TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with a single unique value:  ['blockTradeIndicator', 'customBasketIndicator', 'packageIndicator', 'postPricedSwapIndicator', 'primeBrokerageTransactionIndicator', 'CurrencyIV', 'NominalIV', 'effectiveDate_hour', 'effectiveDate_minute', 'effectiveDate_second', 'expirationDate_hour', 'expirationDate_minute', 'expirationDate_second']\n",
      "Columns with NaN or infinite values before scaling:  []\n",
      "All columns in t_data_matched are equal to train_encoded_columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/30rpw85n53d2zxnt_c5qnrx40000gn/T/ipykernel_88171/180097037.py:94: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(original_dtype):\n",
      "/var/folders/rq/30rpw85n53d2zxnt_c5qnrx40000gn/T/ipykernel_88171/180097037.py:94: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(original_dtype):\n",
      "/var/folders/rq/30rpw85n53d2zxnt_c5qnrx40000gn/T/ipykernel_88171/180097037.py:94: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(original_dtype):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "folder_path = your_path + '6.Active Data/Test Data/'\n",
    "# Get file in the folder\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# MAC issue\n",
    "files = [f for f in files if f != '.DS_Store']\n",
    "\n",
    "test_data_name = files[0][:-4]\n",
    "\n",
    "# Load the data\n",
    "t_data = pd.read_csv(your_path + '3.Cash_Risk/' + test_data_name+'_Cash_Risk.csv')\n",
    "\n",
    "# Drop 'disseminationTimestamp' and its derived columns\n",
    "#t_data = t_data.drop(columns=['disseminationTimestamp','sDRreceiptTimestamp','disseminationIdentifier'])\n",
    "\n",
    "# Step 1: Handle Missing Data\n",
    "t_data_filled = t_data.fillna(0)\n",
    "\n",
    "# Step 2: Extract Information from DateTime Columns\n",
    "t_datetime_columns = ['effectiveDate', 'expirationDate']\n",
    "\n",
    "def extract_date_features(df, column):\n",
    "    df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "    df[column + '_year'] = df[column].dt.year\n",
    "    df[column + '_month'] = df[column].dt.month\n",
    "    df[column + '_day'] = df[column].dt.day\n",
    "    df[column + '_hour'] = df[column].dt.hour\n",
    "    df[column + '_minute'] = df[column].dt.minute\n",
    "    df[column + '_second'] = df[column].dt.second\n",
    "    df[column + '_weekday'] = df[column].dt.weekday\n",
    "    # Drop the original datetime column\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df\n",
    "\n",
    "for col in t_datetime_columns:\n",
    "    t_data_filled = extract_date_features(t_data_filled, col)\n",
    "\n",
    "# Convert boolean columns to binary (1/0) before one-hot encoding other categorical columns\n",
    "t_data_filled = t_data_filled*1\n",
    "\n",
    "def compare_item(row,leg1,leg2):\n",
    "    return 1 if row[leg1] == row[leg2] else 0\n",
    "\n",
    "cur = t_data_filled.apply(lambda row: compare_item(row, 'leg1NotionalCurrency','leg2NotionalCurrency'), axis=1)\n",
    "nom = t_data_filled.apply(lambda row: compare_item(row, 'leg1NotionalAmount','leg2NotionalAmount'), axis=1)\n",
    "\n",
    "# Step 3: Encode Categorical Variables\n",
    "t_categorical_columns = t_data_filled.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "t_categorical_columns = [col for col in t_categorical_columns if col not in t_datetime_columns]\n",
    "t_data_encoded = pd.get_dummies(t_data_filled, columns=t_categorical_columns, drop_first=True)\n",
    "\n",
    "# Step 3.5: Identify and Drop Single-Value Columns\n",
    "t_single_value_columns = t_data_encoded.columns[t_data_encoded.nunique() == 1].tolist()\n",
    "\n",
    "# Optionally print these columns and their unique values\n",
    "print(\"Columns with a single unique value: \", t_single_value_columns)\n",
    "\n",
    "# Drop the single-value columns\n",
    "t_data_encoded = t_data_encoded.drop(columns=t_single_value_columns)\n",
    "#t_data_encoded = t_data_encoded.fillna(0)\n",
    "\n",
    "t_data_encoded['CurrencyIV'] = cur\n",
    "t_data_encoded['NominalIV'] = nom\n",
    "\n",
    "# Define numerical_columns here, after all column dropping and adding has occurred\n",
    "t_numerical_columns = t_data_encoded.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Check for NaN or infinite values in numerical columns before scaling\n",
    "t_nan_inf_columns = t_data_encoded[t_numerical_columns].columns[t_data_encoded[t_numerical_columns].isna().any() | np.isinf(t_data_encoded[t_numerical_columns]).any()].tolist()\n",
    "print(\"Columns with NaN or infinite values before scaling: \", t_nan_inf_columns)\n",
    "\n",
    "def match_columns(new_data, train_encoded_columns, original_dtypes):\n",
    "    missing_cols = {col: 0 for col in train_encoded_columns if col not in new_data.columns}\n",
    "    extra_cols = [col for col in new_data.columns if col not in train_encoded_columns]\n",
    "    \n",
    "    # Add missing columns with 0s\n",
    "    new_data = pd.concat([new_data, pd.DataFrame(missing_cols, index=new_data.index)], axis=1)\n",
    "    \n",
    "    # Remove extra columns\n",
    "    new_data = new_data.drop(columns=extra_cols)\n",
    "    \n",
    "    # Ensure columns are in the same order as training data\n",
    "    new_data = new_data[train_encoded_columns]\n",
    "    \n",
    "    # Convert columns back to their original type\n",
    "    for col, original_dtype in original_dtypes.items():\n",
    "        if col in new_data.columns:  # Ensure the column exists in new_data\n",
    "            if pd.api.types.is_categorical_dtype(original_dtype):\n",
    "                new_data[col] = pd.Categorical(new_data[col])\n",
    "            else:\n",
    "                new_data[col] = new_data[col].astype(original_dtype)\n",
    "            \n",
    "    \n",
    "    return new_data\n",
    "\n",
    "\n",
    "# Apply the function\n",
    "t_data_matched = match_columns(t_data_encoded, train_encoded_columns, original_dtypes)\n",
    "\n",
    "# Check if all columns in t_data_matched are equal to train_encoded_columns\n",
    "if list(t_data_matched.columns) == list(train_encoded_columns):\n",
    "\n",
    "    print(\"All columns in t_data_matched are equal to train_encoded_columns.\")\n",
    "else:\n",
    "    print(\"Columns in t_data_matched and train_encoded_columns do not match.\")\n",
    "\n",
    "    # Check if the columns are the same but in a different order\n",
    "    if set(t_data_matched.columns) == set(train_encoded_columns):\n",
    "        print(\"The datasets have the same columns, but they are in a different order.\")\n",
    "        \n",
    "        # You can reorder the columns in t_data_matched to match train_encoded_columns\n",
    "        t_data_matched = t_data_matched[train_encoded_columns]\n",
    "        print(\"Columns in t_data_matched reordered to match train_encoded_columns.\")\n",
    "    else:\n",
    "        # Identify and print the mismatched columns\n",
    "        extra_cols = set(t_data_matched.columns) - set(train_encoded_columns)\n",
    "        missing_cols = set(train_encoded_columns) - set(t_data_matched.columns)\n",
    "        print(\"Extra columns in t_data_matched: \", extra_cols)\n",
    "        print(\"Missing columns in t_data_matched: \", missing_cols)\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Load the saved scaler\n",
    "scaler = joblib.load(your_path +'0.Code/robust_scaler.pkl')\n",
    "\n",
    "# Scale the numerical columns of t_data_matched using the 'numerical_columns' from the training data\n",
    "t_data_matched[numerical_columns] = scaler.transform(t_data_matched[numerical_columns])\n",
    "\n",
    "t_data_matched.to_csv(your_path + '4.Scaled/' + test_data_name + '_Scaled.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
