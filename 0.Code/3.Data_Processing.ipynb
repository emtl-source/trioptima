{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 1 - PROCESS CLEAN DATA (TRAINING) AND SAVE THE SCALER AS robust_scaler.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with a single unique value:  ['customBasketIndicator', 'packageIndicator', 'postPricedSwapIndicator', 'CurrencyIV', 'NominalIV', 'effectiveDate_hour', 'effectiveDate_minute', 'effectiveDate_second', 'eventDateTime_year', 'eventDateTime_month', 'eventDateTime_day', 'eventDateTime_hour', 'eventDateTime_minute', 'eventDateTime_second', 'eventDateTime_weekday', 'executionDateTime_year', 'executionDateTime_month', 'executionDateTime_day', 'executionDateTime_hour', 'executionDateTime_minute', 'executionDateTime_second', 'executionDateTime_weekday', 'expirationDate_hour', 'expirationDate_minute', 'expirationDate_second']\n",
      "Index(['leg1NotionalAmount', 'leg2NotionalAmount',\n",
      "       'leg2ResetFrequencyMultiplier', 'leg1FixedRate',\n",
      "       'leg1FixedRatePaymentFrequencyMultiplier',\n",
      "       'leg2FloatingRatePaymentFrequencyMultiplier',\n",
      "       'leg2UnderlierTenorMultiplier', 'leg1NotionalAmountUSD', 'MtM_leg1',\n",
      "       'MtM_leg2',\n",
      "       ...\n",
      "       'leg2UnderlierIDSource_CDOR', 'leg2UnderlierIDSource_COMPOUND',\n",
      "       'leg2UnderlierIDSource_EURIBOR', 'leg2UnderlierIDSource_FPML',\n",
      "       'leg2UnderlierIDSource_OIS Compound', 'leg2UnderlierTenorPeriod_YEAR',\n",
      "       'leg2ResetFrequencyPeriod_MNTH', 'leg2ResetFrequencyPeriod_YEAR',\n",
      "       'CurrencyIV', 'NominalIV'],\n",
      "      dtype='object', length=166)\n",
      "Columns with NaN or infinite values before scaling:  []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "#r'C:/Users/gusta/Documents/KTH/TriOptima/trioptima/trioptima/'\n",
    "#'/Users/elliotlindestam/Documents/Skola/Indek icloud/trioptima/'\n",
    "your_path = r'C:/Users/gusta/Documents/KTH/TriOptima/trioptima/trioptima/'\n",
    "folder_path = your_path + '6.Active Data/Train Model Data/'\n",
    "# Get file in the folder\n",
    "files = os.listdir(folder_path)\n",
    "# MAC issue\n",
    "files = [f for f in files if f != '.DS_Store']\n",
    "training_data_name = files[0][:-4]\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(your_path + \"3.Cash_Risk/\" + training_data_name+'_Cash_Risk.csv')\n",
    "\n",
    "# Step 1: Handle Missing Data\n",
    "data_filled = data.fillna(0)\n",
    "\n",
    "# Step 2: Extract Information from DateTime Columns\n",
    "datetime_columns = ['effectiveDate', 'eventDateTime', 'executionDateTime', 'expirationDate'] # ['effectiveDate', 'expirationDate'] # \n",
    "\n",
    "def extract_date_features(df, column):\n",
    "    df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "    df[column + '_year'] = df[column].dt.year\n",
    "    df[column + '_month'] = df[column].dt.month\n",
    "    df[column + '_day'] = df[column].dt.day\n",
    "    df[column + '_hour'] = df[column].dt.hour\n",
    "    df[column + '_minute'] = df[column].dt.minute\n",
    "    df[column + '_second'] = df[column].dt.second\n",
    "    df[column + '_weekday'] = df[column].dt.weekday\n",
    "    # Drop the original datetime column\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df\n",
    "\n",
    "for col in datetime_columns:\n",
    "    data_filled = extract_date_features(data_filled, col)\n",
    "\n",
    "original_dtypes = data.dtypes.to_dict()\n",
    "\n",
    "# Convert boolean columns to binary (1/0) before one-hot encoding other categorical columns\n",
    "data_filled = data_filled*1\n",
    "\n",
    "def compare_item(row,leg1,leg2):\n",
    "    return 1 if row[leg1] == row[leg2] else 0\n",
    "\n",
    "cur = data_filled.apply(lambda row: compare_item(row, 'leg1NotionalCurrency','leg2NotionalCurrency'), axis=1)\n",
    "nom = data_filled.apply(lambda row: compare_item(row, 'leg1NotionalAmount','leg2NotionalAmount'), axis=1)\n",
    "\n",
    "# Step 3: Encode Categorical Variables\n",
    "categorical_columns = data_filled.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "categorical_columns = [col for col in categorical_columns if col not in datetime_columns]\n",
    "data_encoded = pd.get_dummies(data_filled, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Step 3.5: Identify and Drop Single-Value Columns\n",
    "single_value_columns = data_encoded.columns[data_encoded.nunique() == 1].tolist()\n",
    "\n",
    "# Optionally print these columns and their unique values\n",
    "print(\"Columns with a single unique value: \", single_value_columns)\n",
    "\n",
    "# Drop the single-value columns\n",
    "data_encoded = data_encoded.drop(columns=single_value_columns)\n",
    "data_encoded['CurrencyIV'] = cur\n",
    "data_encoded['NominalIV'] = nom\n",
    "#data_encoded = data_encoded.fillna(0)\n",
    "# Define numerical_columns here, after all column dropping and adding has occurred\n",
    "\n",
    "\n",
    "\n",
    "print(data_encoded.columns)\n",
    "# Create interaction variable\n",
    "\n",
    "\n",
    "numerical_columns = data_encoded.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Check for NaN or infinite values in numerical columns before scaling\n",
    "nan_inf_columns = data_encoded[numerical_columns].columns[data_encoded[numerical_columns].isna().any() | np.isinf(data_encoded[numerical_columns]).any()].tolist()\n",
    "print(\"Columns with NaN or infinite values before scaling: \", nan_inf_columns)\n",
    "\n",
    "# Step 4: Scale/Normalize Data\n",
    "numerical_columns = data_encoded.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "scaler = RobustScaler()\n",
    "data_encoded[numerical_columns] = scaler.fit_transform(data_encoded[numerical_columns])\n",
    "\n",
    "# Save the scaler for future use\n",
    "scaler_filename = 'robust_scaler.pkl'\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "# Preprocessed data is now stored in `data_encoded`, and the scaler is saved as `robust_scaler.pkl`\n",
    "# Save the processed data to a CSV file in this environment\n",
    "data_encoded.to_csv(your_path + '4.Scaled/' + training_data_name + '_Scaled.csv', index=False)\n",
    "\n",
    "train_encoded_columns = data_encoded.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2 - PROCESS THE TEST DATA USING THE SCALER IN TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with a single unique value:  ['customBasketIndicator', 'packageIndicator', 'postPricedSwapIndicator', 'leg1FixedRatePaymentFrequencyMultiplier', 'CurrencyIV', 'NominalIV', 'effectiveDate_hour', 'effectiveDate_minute', 'effectiveDate_second', 'expirationDate_hour', 'expirationDate_minute', 'expirationDate_second']\n",
      "Columns with NaN or infinite values before scaling:  []\n",
      "All columns in t_data_matched are equal to train_encoded_columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gusta\\AppData\\Local\\Temp\\ipykernel_26828\\267547197.py:94: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(original_dtype):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "folder_path = your_path + '6.Active Data/Test Data/'\n",
    "# Get file in the folder\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# MAC issue\n",
    "files = [f for f in files if f != '.DS_Store']\n",
    "\n",
    "test_data_name = files[0][:-4]\n",
    "\n",
    "# Load the data\n",
    "t_data = pd.read_csv(your_path + '3.Cash_Risk/' + test_data_name+'_Cash_Risk.csv')\n",
    "\n",
    "# Drop 'disseminationTimestamp' and its derived columns\n",
    "t_data = t_data.drop(columns=['disseminationTimestamp','sDRreceiptTimestamp','disseminationIdentifier'])\n",
    "\n",
    "# Step 1: Handle Missing Data\n",
    "t_data_filled = t_data.fillna(0)\n",
    "\n",
    "# Step 2: Extract Information from DateTime Columns\n",
    "t_datetime_columns = ['effectiveDate', 'expirationDate']\n",
    "\n",
    "def extract_date_features(df, column):\n",
    "    df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "    df[column + '_year'] = df[column].dt.year\n",
    "    df[column + '_month'] = df[column].dt.month\n",
    "    df[column + '_day'] = df[column].dt.day\n",
    "    df[column + '_hour'] = df[column].dt.hour\n",
    "    df[column + '_minute'] = df[column].dt.minute\n",
    "    df[column + '_second'] = df[column].dt.second\n",
    "    df[column + '_weekday'] = df[column].dt.weekday\n",
    "    # Drop the original datetime column\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df\n",
    "\n",
    "for col in t_datetime_columns:\n",
    "    t_data_filled = extract_date_features(t_data_filled, col)\n",
    "\n",
    "# Convert boolean columns to binary (1/0) before one-hot encoding other categorical columns\n",
    "t_data_filled = t_data_filled*1\n",
    "\n",
    "def compare_item(row,leg1,leg2):\n",
    "    return 1 if row[leg1] == row[leg2] else 0\n",
    "\n",
    "cur = t_data_filled.apply(lambda row: compare_item(row, 'leg1NotionalCurrency','leg2NotionalCurrency'), axis=1)\n",
    "nom = t_data_filled.apply(lambda row: compare_item(row, 'leg1NotionalAmount','leg2NotionalAmount'), axis=1)\n",
    "\n",
    "# Step 3: Encode Categorical Variables\n",
    "t_categorical_columns = t_data_filled.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "t_categorical_columns = [col for col in t_categorical_columns if col not in t_datetime_columns]\n",
    "t_data_encoded = pd.get_dummies(t_data_filled, columns=t_categorical_columns, drop_first=True)\n",
    "\n",
    "# Step 3.5: Identify and Drop Single-Value Columns\n",
    "t_single_value_columns = t_data_encoded.columns[t_data_encoded.nunique() == 1].tolist()\n",
    "\n",
    "# Optionally print these columns and their unique values\n",
    "print(\"Columns with a single unique value: \", t_single_value_columns)\n",
    "\n",
    "# Drop the single-value columns\n",
    "t_data_encoded = t_data_encoded.drop(columns=t_single_value_columns)\n",
    "#t_data_encoded = t_data_encoded.fillna(0)\n",
    "\n",
    "t_data_encoded['CurrencyIV'] = cur\n",
    "t_data_encoded['NominalIV'] = nom\n",
    "\n",
    "# Define numerical_columns here, after all column dropping and adding has occurred\n",
    "t_numerical_columns = t_data_encoded.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Check for NaN or infinite values in numerical columns before scaling\n",
    "t_nan_inf_columns = t_data_encoded[t_numerical_columns].columns[t_data_encoded[t_numerical_columns].isna().any() | np.isinf(t_data_encoded[t_numerical_columns]).any()].tolist()\n",
    "print(\"Columns with NaN or infinite values before scaling: \", t_nan_inf_columns)\n",
    "\n",
    "def match_columns(new_data, train_encoded_columns, original_dtypes):\n",
    "    missing_cols = {col: 0 for col in train_encoded_columns if col not in new_data.columns}\n",
    "    extra_cols = [col for col in new_data.columns if col not in train_encoded_columns]\n",
    "    \n",
    "    # Add missing columns with 0s\n",
    "    new_data = pd.concat([new_data, pd.DataFrame(missing_cols, index=new_data.index)], axis=1)\n",
    "    \n",
    "    # Remove extra columns\n",
    "    new_data = new_data.drop(columns=extra_cols)\n",
    "    \n",
    "    # Ensure columns are in the same order as training data\n",
    "    new_data = new_data[train_encoded_columns]\n",
    "    \n",
    "    # Convert columns back to their original type\n",
    "    for col, original_dtype in original_dtypes.items():\n",
    "        if col in new_data.columns:  # Ensure the column exists in new_data\n",
    "            if pd.api.types.is_categorical_dtype(original_dtype):\n",
    "                new_data[col] = pd.Categorical(new_data[col])\n",
    "            else:\n",
    "                new_data[col] = new_data[col].astype(original_dtype)\n",
    "            \n",
    "    \n",
    "    return new_data\n",
    "\n",
    "\n",
    "# Apply the function\n",
    "t_data_matched = match_columns(t_data_encoded, train_encoded_columns, original_dtypes)\n",
    "\n",
    "# Check if all columns in t_data_matched are equal to train_encoded_columns\n",
    "if list(t_data_matched.columns) == list(train_encoded_columns):\n",
    "\n",
    "    print(\"All columns in t_data_matched are equal to train_encoded_columns.\")\n",
    "else:\n",
    "    print(\"Columns in t_data_matched and train_encoded_columns do not match.\")\n",
    "\n",
    "    # Check if the columns are the same but in a different order\n",
    "    if set(t_data_matched.columns) == set(train_encoded_columns):\n",
    "        print(\"The datasets have the same columns, but they are in a different order.\")\n",
    "        \n",
    "        # You can reorder the columns in t_data_matched to match train_encoded_columns\n",
    "        t_data_matched = t_data_matched[train_encoded_columns]\n",
    "        print(\"Columns in t_data_matched reordered to match train_encoded_columns.\")\n",
    "    else:\n",
    "        # Identify and print the mismatched columns\n",
    "        extra_cols = set(t_data_matched.columns) - set(train_encoded_columns)\n",
    "        missing_cols = set(train_encoded_columns) - set(t_data_matched.columns)\n",
    "        print(\"Extra columns in t_data_matched: \", extra_cols)\n",
    "        print(\"Missing columns in t_data_matched: \", missing_cols)\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Load the saved scaler\n",
    "scaler = joblib.load(your_path +'0.Code/robust_scaler.pkl')\n",
    "\n",
    "# Scale the numerical columns of t_data_matched using the 'numerical_columns' from the training data\n",
    "t_data_matched[numerical_columns] = scaler.transform(t_data_matched[numerical_columns])\n",
    "\n",
    "t_data_matched.to_csv(your_path + '4.Scaled/' + test_data_name + '_Scaled.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
