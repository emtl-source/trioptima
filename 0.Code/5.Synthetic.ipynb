{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OskarHansson\\AppData\\Local\\Temp\\ipykernel_15720\\660110282.py:21: DtypeWarning: Columns (1,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "# CREATE SYNTHETIC DATA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a template DataFrame with the same structure as your original data\n",
    "# Replace this with the actual column names and types from your original data\n",
    "\n",
    "your_path = 'C:/Users/OskarHansson/trioptima/'\n",
    "file_path = your_path + '3.Cash_Risk/Modified_IRS_All_Train_VF_Cash_Risk.csv'\n",
    "\n",
    "cols_to_modify = [\n",
    "\"total_delta\",\n",
    "\"MtM_leg2\",\n",
    "\"MtM_leg1\",\n",
    "\"leg1NotionalAmountUSD\",\n",
    "\"leg1FixedRate\",\n",
    "\"leg2NotionalAmount\",\n",
    "\"leg1NotionalAmount\"]\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Number of duplicates\n",
    "num_duplicates = 1\n",
    "\n",
    "# List to store duplicated rows\n",
    "synthetic_data = []\n",
    "\n",
    "# Loop through each row and create duplicates with noise\n",
    "i = 0\n",
    "for index, row in data.iterrows():\n",
    "    for _ in range(num_duplicates):\n",
    "        new_row = row.copy()\n",
    "        i += 1\n",
    "\n",
    "        for col_to_modify in cols_to_modify:\n",
    "            # Create a new row by adding noise to the data    \n",
    "            new_row[col_to_modify] += np.random.normal(0, abs(new_row[col_to_modify]) * 0.05)\n",
    "            \n",
    "        synthetic_data.append(new_row)\n",
    "\n",
    "\n",
    "# Concatenate the original data with the synthetic data\n",
    "synthetic_data = pd.concat([data, pd.DataFrame(synthetic_data)], ignore_index=True)\n",
    "\n",
    "# Save the synthetic data to a new CSV file\n",
    "synthetic_data.to_csv(your_path + \"3.Cash_Risk/synthetic_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OskarHansson\\AppData\\Local\\Temp\\ipykernel_15720\\645665953.py:12: DtypeWarning: Columns (1,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(your_path + \"3.Cash_Risk\\Modified_IRS_All_Train_VF_Cash_Risk.csv\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\OskarHansson\\trioptima\\0.Code\\5.Synthetic.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/OskarHansson/trioptima/0.Code/5.Synthetic.ipynb#X65sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m df\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/OskarHansson/trioptima/0.Code/5.Synthetic.ipynb#X65sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m datetime_columns:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/OskarHansson/trioptima/0.Code/5.Synthetic.ipynb#X65sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     data_filled \u001b[39m=\u001b[39m extract_date_features(data_filled, col)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/OskarHansson/trioptima/0.Code/5.Synthetic.ipynb#X65sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m original_dtypes \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mdtypes\u001b[39m.\u001b[39mto_dict()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/OskarHansson/trioptima/0.Code/5.Synthetic.ipynb#X65sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Convert boolean columns to binary (1/0) before one-hot encoding other categorical columns\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\OskarHansson\\trioptima\\0.Code\\5.Synthetic.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/OskarHansson/trioptima/0.Code/5.Synthetic.ipynb#X65sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_date_features\u001b[39m(df, column):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/OskarHansson/trioptima/0.Code/5.Synthetic.ipynb#X65sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     df[column] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(df[column], errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcoerce\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/OskarHansson/trioptima/0.Code/5.Synthetic.ipynb#X65sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     df[column \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_year\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[column]\u001b[39m.\u001b[39;49mdt\u001b[39m.\u001b[39myear\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/OskarHansson/trioptima/0.Code/5.Synthetic.ipynb#X65sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     df[column \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_month\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[column]\u001b[39m.\u001b[39mdt\u001b[39m.\u001b[39mmonth\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/OskarHansson/trioptima/0.Code/5.Synthetic.ipynb#X65sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     df[column \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_day\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[column]\u001b[39m.\u001b[39mdt\u001b[39m.\u001b[39mday\n",
      "File \u001b[1;32mc:\\Users\\OskarHansson\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5895\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5896\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5897\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5898\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5899\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5900\u001b[0m ):\n\u001b[0;32m   5901\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "File \u001b[1;32mc:\\Users\\OskarHansson\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\accessor.py:182\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     \u001b[39m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessor\n\u001b[1;32m--> 182\u001b[0m accessor_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor(obj)\n\u001b[0;32m    183\u001b[0m \u001b[39m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[39m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[39m# NDFrame\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32mc:\\Users\\OskarHansson\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\accessors.py:512\u001b[0m, in \u001b[0;36mCombinedDatetimelikeProperties.__new__\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[39melif\u001b[39;00m is_period_dtype(data\u001b[39m.\u001b[39mdtype):\n\u001b[0;32m    510\u001b[0m     \u001b[39mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[1;32m--> 512\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "#r'C:/Users/gusta/Documents/KTH/TriOptima/trioptima/trioptima/'\n",
    "#'/Users/elliotlindestam/Documents/Skola/Indek icloud/trioptima/'\n",
    "your_path = 'C:/Users/OskarHansson/trioptima/'\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(your_path + \"3.Cash_Risk\\Modified_IRS_All_Train_VF_Cash_Risk.csv\")\n",
    "\n",
    "# Step 1: Handle Missing Data\n",
    "data_filled = data.fillna(0)\n",
    "\n",
    "# Step 2: Extract Information from DateTime Columns\n",
    "datetime_columns = ['effectiveDate', 'executionDateTime', 'expirationDate'] # ['effectiveDate', 'expirationDate', 'eventDateTime',] # \n",
    "\n",
    "def extract_date_features(df, column):\n",
    "    df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "    df[column + '_year'] = df[column].dt.year\n",
    "    df[column + '_month'] = df[column].dt.month\n",
    "    df[column + '_day'] = df[column].dt.day\n",
    "    df[column + '_hour'] = df[column].dt.hour\n",
    "    df[column + '_minute'] = df[column].dt.minute\n",
    "    df[column + '_second'] = df[column].dt.second\n",
    "    df[column + '_weekday'] = df[column].dt.weekday\n",
    "    # Drop the original datetime column\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df\n",
    "\n",
    "for col in datetime_columns:\n",
    "    data_filled = extract_date_features(data_filled, col)\n",
    "\n",
    "original_dtypes = data.dtypes.to_dict()\n",
    "\n",
    "# Convert boolean columns to binary (1/0) before one-hot encoding other categorical columns\n",
    "data_filled = data_filled*1\n",
    "\n",
    "def compare_item(row,leg1,leg2):\n",
    "    return 1 if row[leg1] == row[leg2] else 0\n",
    "\n",
    "cur = data_filled.apply(lambda row: compare_item(row, 'leg1NotionalCurrency','leg2NotionalCurrency'), axis=1)\n",
    "nom = data_filled.apply(lambda row: compare_item(row, 'leg1NotionalAmount','leg2NotionalAmount'), axis=1)\n",
    "\n",
    "# Step 3: Encode Categorical Variables\n",
    "categorical_columns = data_filled.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "categorical_columns = [col for col in categorical_columns if col not in datetime_columns]\n",
    "data_encoded = pd.get_dummies(data_filled, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Step 3.5: Identify and Drop Single-Value Columns\n",
    "single_value_columns = data_encoded.columns[data_encoded.nunique() == 1].tolist()\n",
    "\n",
    "# Optionally print these columns and their unique values\n",
    "print(\"Columns with a single unique value: \", single_value_columns)\n",
    "\n",
    "# Drop the single-value columns\n",
    "data_encoded = data_encoded.drop(columns=single_value_columns)\n",
    "data_encoded['CurrencyIV'] = cur\n",
    "data_encoded['NominalIV'] = nom\n",
    "#data_encoded = data_encoded.fillna(0)\n",
    "# Define numerical_columns here, after all column dropping and adding has occurred\n",
    "\n",
    "\n",
    "\n",
    "print(data_encoded.columns)\n",
    "# Create interaction variable\n",
    "\n",
    "\n",
    "numerical_columns = data_encoded.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Check for NaN or infinite values in numerical columns before scaling\n",
    "nan_inf_columns = data_encoded[numerical_columns].columns[data_encoded[numerical_columns].isna().any() | np.isinf(data_encoded[numerical_columns]).any()].tolist()\n",
    "print(\"Columns with NaN or infinite values before scaling: \", nan_inf_columns)\n",
    "\n",
    "# Step 4: Scale/Normalize Data\n",
    "numerical_columns = data_encoded.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "scaler = RobustScaler()\n",
    "data_encoded[numerical_columns] = scaler.fit_transform(data_encoded[numerical_columns])\n",
    "\n",
    "# Save the scaler for future use\n",
    "#scaler_filename = 'robust_scaler.pkl'\n",
    "#joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "# Preprocessed data is now stored in `data_encoded`, and the scaler is saved as `robust_scaler.pkl`\n",
    "# Save the processed data to a CSV file in this environment\n",
    "data_encoded.to_csv(your_path + '4.Scaled/synthetic_data.csv', index=False)\n",
    "\n",
    "train_encoded_columns = data_encoded.columns\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
