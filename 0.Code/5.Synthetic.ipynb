{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE SYNTHETIC DATA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a template DataFrame with the same structure as your original data\n",
    "# Replace this with the actual column names and types from your original data\n",
    "\n",
    "your_path = '/Users/elliotlindestam/Documents/Skola/Indek icloud/trioptima/'\n",
    "file_path = your_path + '2.Cleaned/Modified_IRS_All_Train_VF_Cleaned.csv'\n",
    "\n",
    "cols_to_modify = [\n",
    "\"leg1FixedRate\",\n",
    "\"leg1NotionalAmount\"]\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Number of duplicates\n",
    "num_duplicates = 1\n",
    "\n",
    "# List to store duplicated rows\n",
    "synthetic_data = []\n",
    "\n",
    "# Loop through each row and create duplicates with noise\n",
    "i = 0\n",
    "for index, row in data.iterrows():\n",
    "    for _ in range(num_duplicates):\n",
    "        new_row = row.copy()\n",
    "        i += 1\n",
    "\n",
    "        for col_to_modify in cols_to_modify:\n",
    "            # Create a new row by adding noise to the data  \n",
    "            if col_to_modify == 'leg1NotionalAmount':\n",
    "                noise = np.random.normal(0, abs(new_row[col_to_modify]) * 0.05)\n",
    "                new_row[col_to_modify] += noise\n",
    "                new_row['leg2NotionalAmount'] +=noise\n",
    "            else:\n",
    "                new_row[col_to_modify] += np.random.normal(0, abs(new_row[col_to_modify]) * 0.05)\n",
    "            \n",
    "        synthetic_data.append(new_row)\n",
    "\n",
    "\n",
    "# Concatenate the original data with the synthetic data\n",
    "synthetic_data = pd.concat([data, pd.DataFrame(synthetic_data)], ignore_index=True)\n",
    "\n",
    "# Save the synthetic data to a new CSV file\n",
    "synthetic_data.to_csv(your_path + \"2.Cleaned/synthetic_data_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic cash risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/30rpw85n53d2zxnt_c5qnrx40000gn/T/ipykernel_2472/3568669447.py:144: DtypeWarning: Columns (1,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(your_path + \"2.Cleaned/synthetic_data_cleaned.csv\")\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Paths\n",
    "#r'C:/Users/gusta/Documents/KTH/TriOptima/trioptima/trioptima/'\n",
    "#'/Users/elliotlindestam/Documents/Skola/Indek icloud/trioptima/'\n",
    "\n",
    "# 2. Configuration\n",
    "your_path = r'/Users/elliotlindestam/Documents/Skola/Indek icloud/trioptima/'\n",
    "train_folder_path = your_path + '6.Active Data/Train Model Data/'\n",
    "test_folder_path = your_path + '6.Active Data/Test Data/'\n",
    "\n",
    "# 3. Functions\n",
    "def convert_to_usd(row,exchange_rates):\n",
    "    return row['leg1NotionalAmount'] * exchange_rates.get(row['leg1NotionalCurrency'], 1)\n",
    "\n",
    "def calculate_cashflows(row, period, freq):\n",
    "    cashflows = []\n",
    "    start_date = row['effectiveDate']\n",
    "    end_date = row['expirationDate']\n",
    "    \n",
    "    if row[period] == 'MNTH':\n",
    "        delta = relativedelta(months=row[freq])\n",
    "    elif row[period] == 'YEAR':\n",
    "        delta = relativedelta(years=row[freq])\n",
    "    elif row[period] == 'DAIL':\n",
    "        delta = timedelta(days=row[freq])\n",
    "    elif row[period] == 'QTR':\n",
    "        delta = relativedelta(months=3)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown frequency period: {row[period]}\")\n",
    "    \n",
    "    while start_date <= end_date:\n",
    "        cashflows.append(start_date)\n",
    "        start_date += delta\n",
    "\n",
    "    return cashflows\n",
    "\n",
    "def convert_to_years(value):\n",
    "    \n",
    "    if \"M\" in value:\n",
    "        return int(str(value)[:-1])/12\n",
    "    elif \"Y\" in value:\n",
    "        return int(str(value)[:-1])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def floating_rate(years_to_payout, float_rates_df, bump):\n",
    "    differences = float_rates_df['Tenor'].sub(years_to_payout).abs()\n",
    "    nearest_index = differences.idxmin()\n",
    "    return (float_rates_df.loc[nearest_index, 'Rate'] + bump)/ 100\n",
    "\n",
    "def calculate_discounted_cashflow(row, rate_type, bump, float_rates_df_list):\n",
    "    total_discounted_cashflow = 0\n",
    "    \n",
    "    # CHECK\n",
    "    discounted_cashflow_list = []\n",
    "    cashflow_list = []\n",
    "    factors = []\n",
    "    pr_list = []\n",
    "    discount_rate = []\n",
    "\n",
    "    ibor = row['leg2UnderlierID']\n",
    "    for item in float_rates_df_list:\n",
    "        if item[0] == ibor:\n",
    "            float_rates_df = item[1]\n",
    "            break \n",
    "    \n",
    "    for date in row['cashflow_dates']:\n",
    "\n",
    "        if len(cashflow_list) == 0:\n",
    "\n",
    "            time_difference = (date - datetime.now()).days / 365.0\n",
    "            tenor = time_difference ### MONTHS IS TO MATCH THE INDEX IN OUR IBOR FILE\n",
    "            float_rate = floating_rate(tenor,float_rates_df, bump)\n",
    "            factor = (1 + float_rate)**time_difference\n",
    "            factors.append(factor)\n",
    "            discount_rate.append([tenor,1/ factor])\n",
    "\n",
    "            if rate_type == 'float':\n",
    "                payout_rate = float_rate\n",
    "                cashflow = row['leg1NotionalAmount'] * payout_rate\n",
    "                \n",
    "            if rate_type == 'fixed':\n",
    "                payout_rate = row['leg1FixedRate']\n",
    "                cashflow = row['leg1NotionalAmount'] * payout_rate\n",
    "                \n",
    "            discounted_cashflow = cashflow / factor\n",
    "        \n",
    "        else:\n",
    "            time_difference = ((date-row['cashflow_dates'][len(factors)-1])).days/365\n",
    "            tenor = (date - datetime.now()).days / 365\n",
    "            float_rate = floating_rate(tenor,float_rates_df,bump)\n",
    "            prev_factor = factors[-1]\n",
    "            factor = prev_factor * (1 + payout_rate) ** time_difference\n",
    "            factors.append(factor)\n",
    "            discount_rate.append([tenor,1/ factor])\n",
    "            if rate_type == 'float':    \n",
    "                payout_rate = float_rate\n",
    "                cashflow = row['leg1NotionalAmount'] * payout_rate\n",
    "            \n",
    "            if rate_type == 'fixed':\n",
    "                payout_rate = row['leg1FixedRate']\n",
    "                cashflow = row['leg1NotionalAmount'] * row['leg1FixedRate']\n",
    "                \n",
    "            discounted_cashflow = cashflow / factor\n",
    "        pr_list.append(payout_rate)\n",
    "        total_discounted_cashflow += discounted_cashflow\n",
    "        \n",
    "        # CHECK\n",
    "        cashflow_list.append(cashflow)\n",
    "        discounted_cashflow_list.append(discounted_cashflow)\n",
    "    \n",
    "    # CHECK \n",
    "    #print('Total discounted CF '+rate_type+': '+str(total_discounted_cashflow))\n",
    "    #print(rate_type + ' cashflow list ' + str((cashflow_list)))\n",
    "    #print(rate_type + ' '+ibor+' discount rates ' + str((discount_rate)))\n",
    "    #print('\\n')\n",
    "    \n",
    "    #print(rate_type + ' discounted cashflow list ' + str(discounted_cashflow_list) )\n",
    "    #print(rate_type + ' discount factors ' + str((factors)))\n",
    "    #print('\\n')\n",
    "    return total_discounted_cashflow\n",
    "\n",
    "def fx_rates (your_path):\n",
    "    exchange_rates_df = pd.read_csv(your_path + '7.IBOR/exchange_rates.csv')\n",
    "    exchange_rates = dict(zip(exchange_rates_df['Currency'], exchange_rates_df['Rate_to_USD']))\n",
    "    return exchange_rates\n",
    "\n",
    "def fl_df (your_path, ibor):\n",
    "    ibor_df = pd.read_csv(your_path + '7.IBOR/' + ibor + '.csv')\n",
    "    ibor_df['Tenor'] = ibor_df['Tenor'].apply(convert_to_years) ## MAKE SURE ALL FILES ARE CODED THE SAME\n",
    "    float_rates_df = ibor_df[['Tenor', 'Rate']].copy()\n",
    "    return float_rates_df\n",
    "\n",
    "def main (folder_path, your_path, bump):\n",
    "    # 4. Data Loading\n",
    "    # Load trade data\n",
    "    \n",
    "    data = pd.read_csv(your_path + \"2.Cleaned/synthetic_data_cleaned.csv\")\n",
    "\n",
    "    # Load exchange rates\n",
    "    fx_df = fx_rates(your_path)\n",
    "   \n",
    "    # 5. Data Transformation and Filtering\n",
    "    filtered_data = data[\n",
    "        (data['leg1NotionalCurrency'].isin(['EUR', 'USD', 'GBP', 'AUD', 'CAD', 'NZD'])) & \n",
    "        (data['leg1UnderlyingAssetOrContractType'].isin(['Fixed-Floating', 'OIS']))\n",
    "    ]\n",
    "    df = pd.DataFrame(filtered_data)\n",
    "\n",
    "\n",
    "    df['effectiveDate'] = pd.to_datetime(df['effectiveDate'])\n",
    "    df['expirationDate'] = pd.to_datetime(df['expirationDate'])\n",
    "\n",
    "    df['cashflow_dates'] = df.apply(\n",
    "        lambda row: \n",
    "            calculate_cashflows(row, 'leg1FixedRatePaymentFrequencyPeriod', 'leg1FixedRatePaymentFrequencyMultiplier') \n",
    "            if row['leg1UnderlyingAssetOrContractType'] == 'Fixed-Floating' \n",
    "            else (calculate_cashflows(row, 'leg2UnderlierTenorPeriod', 'leg2UnderlierTenorMultiplier') \n",
    "                if row['leg1UnderlyingAssetOrContractType'] == 'OIS' \n",
    "                else None), \n",
    "        axis=1)\n",
    "\n",
    "    # 6. Based on data - Load relevant IBORs to be used as float_rates in operations\n",
    "    float_rates_df_list = []\n",
    "    for ibor in df['leg2UnderlierID'].unique():\n",
    "        float_rates_df_list.append([ibor,fl_df(your_path,ibor)])\n",
    "        \n",
    "    # 7. Main Operations\n",
    "    df['leg1NotionalAmountUSD'] = df.apply(lambda row: convert_to_usd(row, fx_df), axis=1)\n",
    "    df['MtM_leg1'] = df.apply(lambda row: calculate_discounted_cashflow(row, 'fixed', 0, float_rates_df_list), axis=1)\n",
    "    df['MtM_leg2'] = df.apply(lambda row: calculate_discounted_cashflow(row, 'float', 0, float_rates_df_list), axis=1)\n",
    "    df['MtM_leg2_bumped'] = df.apply(lambda row: calculate_discounted_cashflow(row, 'float', bump/100, float_rates_df_list), axis=1)\n",
    "    df['total_delta'] = (df['MtM_leg2'] - df['MtM_leg2_bumped']).abs()\n",
    "    \n",
    "    df = df[df['MtM_leg1'] != 0]\n",
    "\n",
    "\n",
    "    # Creates outliers in the test data if the test file is IRS_2023_ValueTest \n",
    "    print(df['MtM_leg1'])\n",
    "    if data_file == 'IRS_2023_ValueTest' or 'IRS_All_ValueTest':\n",
    "        i = 0\n",
    "        print('OUTLIERS')\n",
    "        for value in df['MtM_leg1']:\n",
    "            \n",
    "            df.at[i, 'MtM_leg1'] = value * random.randint(1,50) / 10000\n",
    "            i += 1\n",
    "    print(df['MtM_leg1'])\n",
    "    # 8. Export/Output\n",
    "    df.drop(columns=['cashflow_dates', 'MtM_leg2_bumped'], inplace=True)\n",
    "    df.to_csv(your_path + '3.Cash_Risk/synthetic_data_Cash_Risk.csv', index=False)\n",
    "\n",
    "# folder path, your path, bump (basis points, used to measure risk), outlier (used to create outliers in test data)\n",
    "main(train_folder_path, your_path,bump=1)\n",
    "#main(test_folder_path, your_path,bump=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "#r'C:/Users/gusta/Documents/KTH/TriOptima/trioptima/trioptima/'\n",
    "#'/Users/elliotlindestam/Documents/Skola/Indek icloud/trioptima/'\n",
    "your_path = '/Users/elliotlindestam/Documents/Skola/Indek icloud/trioptima/'\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(your_path + '3.Cash_Risk/synthetic_data_Cash_Risk.csv')\n",
    "\n",
    "# Step 1: Handle Missing Data\n",
    "data_filled = data.fillna(0)\n",
    "\n",
    "# Step 2: Extract Information from DateTime Columns\n",
    "datetime_columns = ['effectiveDate', 'executionDateTime', 'expirationDate'] # ['effectiveDate', 'expirationDate', 'eventDateTime',] # \n",
    "\n",
    "def extract_date_features(df, column):\n",
    "    df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "    df[column + '_year'] = df[column].dt.year\n",
    "    df[column + '_month'] = df[column].dt.month\n",
    "    df[column + '_day'] = df[column].dt.day\n",
    "    df[column + '_hour'] = df[column].dt.hour\n",
    "    df[column + '_minute'] = df[column].dt.minute\n",
    "    df[column + '_second'] = df[column].dt.second\n",
    "    df[column + '_weekday'] = df[column].dt.weekday\n",
    "    # Drop the original datetime column\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df\n",
    "\n",
    "for col in datetime_columns:\n",
    "    data_filled = extract_date_features(data_filled, col)\n",
    "\n",
    "original_dtypes = data.dtypes.to_dict()\n",
    "\n",
    "# Convert boolean columns to binary (1/0) before one-hot encoding other categorical columns\n",
    "data_filled = data_filled*1\n",
    "\n",
    "def compare_item(row,leg1,leg2):\n",
    "    return 1 if row[leg1] == row[leg2] else 0\n",
    "\n",
    "cur = data_filled.apply(lambda row: compare_item(row, 'leg1NotionalCurrency','leg2NotionalCurrency'), axis=1)\n",
    "nom = data_filled.apply(lambda row: compare_item(row, 'leg1NotionalAmount','leg2NotionalAmount'), axis=1)\n",
    "\n",
    "# Step 3: Encode Categorical Variables\n",
    "categorical_columns = data_filled.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "categorical_columns = [col for col in categorical_columns if col not in datetime_columns]\n",
    "data_encoded = pd.get_dummies(data_filled, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Step 3.5: Identify and Drop Single-Value Columns\n",
    "single_value_columns = data_encoded.columns[data_encoded.nunique() == 1].tolist()\n",
    "\n",
    "# Optionally print these columns and their unique values\n",
    "print(\"Columns with a single unique value: \", single_value_columns)\n",
    "\n",
    "# Drop the single-value columns\n",
    "data_encoded = data_encoded.drop(columns=single_value_columns)\n",
    "data_encoded['CurrencyIV'] = cur\n",
    "data_encoded['NominalIV'] = nom\n",
    "#data_encoded = data_encoded.fillna(0)\n",
    "# Define numerical_columns here, after all column dropping and adding has occurred\n",
    "\n",
    "print(data_encoded.columns)\n",
    "# Create interaction variable\n",
    "\n",
    "numerical_columns = data_encoded.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Check for NaN or infinite values in numerical columns before scaling\n",
    "nan_inf_columns = data_encoded[numerical_columns].columns[data_encoded[numerical_columns].isna().any() | np.isinf(data_encoded[numerical_columns]).any()].tolist()\n",
    "print(\"Columns with NaN or infinite values before scaling: \", nan_inf_columns)\n",
    "\n",
    "# Step 4: Scale/Normalize Data\n",
    "numerical_columns = data_encoded.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "scaler = RobustScaler()\n",
    "data_encoded[numerical_columns] = scaler.fit_transform(data_encoded[numerical_columns])\n",
    "\n",
    "# Save the scaler for future use\n",
    "scaler_filename = 'robust_scaler.pkl'\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "# Preprocessed data is now stored in `data_encoded`, and the scaler is saved as `robust_scaler.pkl`\n",
    "# Save the processed data to a CSV file in this environment\n",
    "data_encoded.to_csv(your_path + '4.Scaled/synthetic_data_Scaled.csv', index=False)\n",
    "\n",
    "train_encoded_columns = data_encoded.columns\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
