{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Directory containing the downloaded CSV files\n",
    "raw_data_directory = os.path.expanduser(\"~/Desktop/Trioptima/Raw data\")\n",
    "\n",
    "# Output directory for categorized CSV files\n",
    "output_directory = os.path.expanduser(\"~/Desktop/Trioptima/Categorized Data\")\n",
    "\n",
    "# List of keywords to categorize CSV files\n",
    "categories = ['RT.FX']  #'RT.CDS', 'RT.EQUITY', 'RT.IRS', RT.FX, RT.COMMODITY\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to process a single file and return a category dataframe\n",
    "def process_file(file_path, category):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, dtype=str)\n",
    "        df.replace('nan', '', inplace=True)\n",
    "        return category, df\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return category, pd.DataFrame()\n",
    "\n",
    "# Iterate through categories\n",
    "for category in categories:\n",
    "    # Dictionary to store data frames for the current category\n",
    "    category_dataframes = {category: pd.DataFrame()}\n",
    "    \n",
    "    category_directory = os.path.join(raw_data_directory, category)\n",
    "    if os.path.exists(category_directory):\n",
    "        processed_files_count = 0  # Reset processed files count for the current category\n",
    "        \n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            # Process files in parallel using ThreadPoolExecutor\n",
    "            futures = []\n",
    "            for filename in os.listdir(category_directory):\n",
    "                if filename.lower().endswith('.csv'):\n",
    "                    file_path = os.path.join(category_directory, filename)\n",
    "                    futures.append(executor.submit(process_file, file_path, category))\n",
    "\n",
    "            # Retrieve results from futures and update category dataframes\n",
    "            for future in futures:\n",
    "                category, df = future.result()\n",
    "                category_dataframes[category] = pd.concat([category_dataframes[category], df], sort=False)\n",
    "\n",
    "                # Increment the processed files count\n",
    "                processed_files_count += 1\n",
    "\n",
    "                # Print the count of processed files after each file\n",
    "                print(f\"Processed {processed_files_count} files from {category} category.\")\n",
    "\n",
    "        # Save data frames to a separate CSV file for the current category\n",
    "        category_output_path = os.path.join(output_directory, f\"{category}_Data.csv\")\n",
    "        category_dataframes[category].to_csv(category_output_path, index=False)\n",
    "        print(f\"{category} Data saved at: {category_output_path}\")\n",
    "    else:\n",
    "        print(f\"Category directory not found: {category_directory}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ftplib import FTP\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# FTP server details\n",
    "ftp_host = \"ftp.cmegroup.com\"\n",
    "ftp_directory = \"/sdr\"\n",
    "\n",
    "# Local directory to save .csv and .zip files\n",
    "local_directory = os.path.expanduser(\"~/Desktop/Trioptima/Raw data\")\n",
    "\n",
    "# Function to download files from FTP server recursively\n",
    "def download_files_from_ftp(ftp, ftp_directory, local_directory):\n",
    "    try:\n",
    "        ftp.cwd(ftp_directory)\n",
    "        files = ftp.nlst()\n",
    "        for file_name in files:\n",
    "            remote_file_path = ftp_directory + '/' + file_name\n",
    "            local_file_path = os.path.join(local_directory, file_name)\n",
    "            if (file_name.lower().endswith('.csv')):\n",
    "                with open(local_file_path, 'wb') as local_file:\n",
    "                    ftp.retrbinary(f\"RETR {remote_file_path}\", local_file.write)\n",
    "                print(f\"Downloaded .csv file: {file_name}\")\n",
    "            elif (file_name.lower().endswith('.zip')):\n",
    "                with open(local_file_path, 'wb') as local_file:\n",
    "                    ftp.retrbinary(f\"RETR {remote_file_path}\", local_file.write)\n",
    "                print(f\"Downloaded .zip file: {file_name}\")\n",
    "                # Extract .zip file\n",
    "                with zipfile.ZipFile(local_file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(local_directory)\n",
    "                print(f\"Extracted .zip file: {file_name}\")\n",
    "                # Delete the original .zip file\n",
    "                os.remove(local_file_path)\n",
    "                print(f\"Deleted .zip file: {file_name}\")\n",
    "            elif '.' not in file_name:\n",
    "                download_files_from_ftp(ftp, remote_file_path, local_directory)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download files: {e}\")\n",
    "\n",
    "# Create local directory if it doesn't exist\n",
    "os.makedirs(local_directory, exist_ok=True)\n",
    "\n",
    "# Connect to FTP server and start downloading files\n",
    "try:\n",
    "    with FTP(ftp_host) as ftp:\n",
    "        ftp.login()  # Log in anonymously\n",
    "        download_files_from_ftp(ftp, ftp_directory, local_directory)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to FTP server: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "x_train, x_test = train_test_split(scaled_data, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Define model\n",
    "input_dim = x_train.shape[1]  # Number of features\n",
    "encoding_dim = 14  # Length of encoded representation. Adjust based on your data.\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\")(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n",
    "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "# Compile model\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train model\n",
    "autoencoder.fit(x_train, x_train, epochs=50, batch_size=32, shuffle=True, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get reconstruction error on the test set\n",
    "reconstructed = autoencoder.predict(x_test)\n",
    "mse = np.mean(np.power(x_test - reconstructed, 2), axis=1)\n",
    "\n",
    "# Set a threshold for anomaly detection\n",
    "threshold = np.quantile(mse, 0.95)  # Adjust the quantile value as needed\n",
    "\n",
    "# Get outliers\n",
    "outliers = x_test[mse > threshold]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
